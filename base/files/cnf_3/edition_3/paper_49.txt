2009 13th International Conference Information Visualisation

A Visual Analytics Model Applied to Lead Generation Library Design in
Drug Discovery
Shawn Konecni, Jianping Zhou, Georges Grinstein
University of Massachusetts Lowell
{skonecni@cs.uml.edu, jzhou@cs.uml.edu, grinstein@cs.uml.edu}
variables for analyzing selected compound subsets
during a single iteration [4]. By integrating a mixture
of clustering, visualization, and computational
methods, useful information can be extracted from the
process to decrease failure rates in future experiments
[17]. Herein, we propose a visual analytics model for
effectively evaluating various algorithms and
compound selection strategies used in creating the lead
generation library. The visual analytics model was
designed to help understand the state of knowledge at
a particular iteration within the selection pipeline to
enable decision making and process customization
before the next iteration is executed. The visual
analytics model incorporated several software tools
built and enhanced within the UMass Lowell
Universal Visualization Platform (UVP) [8].
A
number of new tools were developed as extensions of
our current tools and applied to this problem. They
included enhanced parallel coordinates [9], an
extension to the heatmap, and Cluster Comparison and
Visualization (CComViz) [7].

Abstract
Lead generation library design is an iterative
process of selecting compounds that would make ideal
drug candidates. We propose a visual analytics model
for effective evaluation of various compound selection
strategies. This model incorporates a clustering
strategy and a set of evaluation metrics for
understanding the current state of knowledge to
enable decision making and process customization
between iterations.

1. Introduction
The creation of a lead generation library is an
iterative process involving the discovery of
compounds that exhibit affinity for a particular
biological target [1, 2]. Candidates are selected from a
virtual combinatorial library consisting of enumerated
compounds derived from a high throughput screening
process. These collections potentially contain millions
of compounds, making it impractical to test all of
them. For this reason, subsets of compounds are
selected for synthesis and testing based on certain
established objectives [2, 3].
The process of selecting compound subsets often
requires simultaneous optimization of multiple
objectives [3]. These objectives, which are sometimes
conflicting in nature, need to be satisfied to identify
compounds that have a good chance of becoming
suitable leads [3, 14]. A compound selection model is
incorporated into this optimization process to select
batches of compounds in an iterative fashion [1, 3]. In
a process called active learning, the compound
selection model is repeatedly updated and recomputed
for each iteration based on the accumulated
information gathered from all previous iterations [1].
Visualization methods make it possible to
evaluate a number of objectives along with other
978-0-7695-3733-7/09 $25.00 © 2009 IEEE
DOI 10.1109/IV.2009.75

2. Methods
Creating a lead generation library starts with an
initial set of known compounds that are representative
of the enumerated collection [1, 13]. The starting
compounds are introduced into a compound selection
model for generation of successive batches of
compounds for biological testing. It is important that
the model be efficient in order to minimize cost and
provide useful information early in the process [17].

2.1. Pipeline Simulation
To simulate the screening process, we utilized a
compound selection model consisting of three parts
including an initialization, a batch size, and a
particular selection strategy, most of which is modeled

345

analytics model provided an opportunity to update the
compound selection model before the next iteration
was executed.

in [1]. After clustering the chemical descriptor data,
we selected a number of compounds randomly from
each cluster as a starting point [13]. By labeling
compounds in each cluster, we increased our chances
of including a diverse mixture of positive and negative
examples for each objective [13]. The chemical
descriptor data is described as static because it does
not change throughout the iterative process. These
chemical descriptors are calculated for each compound
before the selection pipeline is executed and cover a
variety of molecular characteristics [15, 27]. We
selected an appropriate batch size to improve
efficiency of the model and improve machine learning
performance. Finally, we utilized multiple compound
selection strategies so that we can compare the
solution sets generated by these methods [4, 14]. Each
selection strategy was used to label a batch of
compounds iteratively until a termination point is
reached. The termination point is typically achieved
when performance of the compound selection model
degrades or a certain number of candidate compounds
are found that possess the necessary chemical
properties to be successful candidates later in the drug
discovery process. It is advantageous to achieve this
termination point as quickly as possible to limit time
and cost associated with biological testing [3].
Compounds were selected for labeling based on
predicted objective values calculated using machine
learning algorithms. These predicted values are
described as dynamic in that they change throughout
the iterative process. For this simulation, we used
random forest regression to predict each objective
value independently [11]. The random forests
algorithm has been shown to perform well for
quantitative and categorical biological data in a
number of cases. The training data consisted of
chemical descriptors and all previously labeled
batches [16]. The predicted results were used to score
and evaluate the unlabeled compounds.

2.3. Clustering Strategy
Clustering was performed using a variety of
clustering algorithms to group compounds based on
similar chemical and objective properties [27]. One
advantage of clustering is that new aggregates were
produced that reduced the density of the dataset by
presenting related groups rather than individual
records for analysis [19].
Through interactive
visualization techniques, the clusters themselves were
explored when further detail was required. Another
advantage is that additional useful information was
obtained for unlabeled data which led to better
computational results.

Fig.1. The visual analytics model is shown as
a five-step cycle. After the data is generated,
the data is clustered using a variety of
clustering algorithms. Multiple visualizations
are generated for evaluation of a particular
selection strategy.
Useful information is
extracted using a predefined set of evaluation
metrics. Before executing the next iteration,
the compound selection model is updated
using accumulated information gathered from
all previous iterations.

2.2. Visual Analytics Model
The pipeline simulation generated a time series
dataset representing a particular strategy. Each unit of
time was represented by a single iteration with each
objective attribute giving associated values for each
time point [10]. As shown in figure 1, our visual
analytics model was realized as a five-step cycle per
iteration starting with a generated dataset. This data
was clustered using a variety of clustering algorithms.
Data points and clusters were projected onto multiple
displays using a set of visualization tools. Based on a
set of evaluation metrics, information was extracted
pertaining to the various algorithms and compound
selection strategies. With this information, the visual

2.4. Visualization Tools
The visual analytics model utilized three
coordinated visualizations within the Universal
Visualization Platform (UVP). The UVP is the fourth
generation of a large extensible visual analytics system
consisting of a variety of tools enabling users to
experiment with new visual analytics techniques and
collaborate with other users. It is similar to many

346

commercial tools (e.g. Spotfire, Tableau, etc.) and
open source tools (e.g. Prefuse, R, etc.). Its advantage
is that it has a number of very high dimensional
visualizations (e.g. RadViz, Vectorized RadViz, and
CComViz) which we can experiment and customize.
Several new tools were developed as extensions of our
current tools. We briefly describe these (verbally) now
and in detail with figures in the case study.

including record and dimension reordering,
aggregation, and summarization [25, 22]. These
functions were implemented to enable quick
identification of interesting patterns and relationships
in the dataset.

2.4.1. Enhanced Parallel Coordinates

CComViz is a visualization tool designed to
support clustering comparison and analysis [7]. In
CComViz, each record is a polyline as in parallel
coordinates with the difference that each record has a
unique index position on an axis, which reduces record
and point overlap. Each axis is a dimension with the
index values corresponding to a particular categorical
cluster assignment. The records in the display are
reordered based on a set of stability metrics in order to
reduce the number of crossing lines. Record
reordering is executed in three major steps, which are
described in detail in [7]. In this way, CComViz
reveals data trends and structures based on cluster and
time series stability.
By adding extra continuous dimensions to the
display, a split layout was achieved with categorical
clustering dimensions on one side and continuous
dimensions on the other. This layout provided a
means to identify associations between cluster stability
and other dimensions.

2.4.3. Cluster
(CComViz)

The most common way to visualize multiple
objective solutions is to display them in a two or three
dimensional scatterplot [4].
To visualize more
objectives simultaneously, other techniques are
required.
Parallel
coordinates
transforms
multidimensional data into a two dimensional pattern
by using vertical axes to represent each dimension [9].
One of the advantages of this tool is that it can handle
an unlimited number of dimensions with the
appropriate display size [20].
The records are
represented as polylines with the axis position based
on the value of the data point scaled to each dimension
[7].
A variation of parallel coordinates integrates
statistical information to summarize a set of polylines
[21, 22]. In a similar fashion, we added distribution
bars to the axes of the display. The distribution bars
were colored based on the number of records that
cross a binned set of axis positions that correspond to
a range of values.
In addition to statistical
information, these bars helped to alleviate the problem
associated with point overlap from multiple records
crossing the same axis position.

Comparison

and

Visualization

2.5. Evaluation Metrics
The visual analytics model consisted of a set of
evaluation metrics that were quantitative and
qualitative in nature. These metrics provided a
framework for extracting useful information from the
selection pipeline.
They included parameter
convergence, cluster stability, time series stability,
diversity, prediction confidence, and chemical
association.

2.4.2. Extension to the Heatmap
The Table Lens is a visualization technique that is
effective for analyzing large tables [6]. Graphical
representations are merged into the table visualization
to help users identify patterns. The many interactive
aspects of the Table Lens include support for drilling
down into several focal areas at once while
maintaining perspective on information structure of
the entire dataset. Associations can be visualized in
the data through the process of sorting a single column
and then analyzing other columns for correlations
[23].
For addressing our problem, we used an
interactive heatmap visualization that is inspired by
the Table Lens. The heat map visualization displays a
grid of cells with each cell colored based on a data
value or computed value [24]. The integration of
computational techniques into the visualization made a
variety of automated functions available to the user

2.5.1. Parameter convergence measured the
simultaneous optimization of various parameters. With
conflicting objectives, a single solution was unlikely
to be optimal for every objective without some sort of
deterioration in one or more other objectives [3, 4].
2.5.2. Cluster stability measured the various
clustering algorithms and identified the most
interesting clusters, data points, and cluster splits.
Stable data records share the main features of their
assigned clusters and are clustered consistently using
multiple algorithms despite small changes in the
dataset [7, 10, 26].

347

We demonstrated the usefulness of our visual
analytics model using two basic selection strategies.
The first selection strategy was to randomly choose a
batch of compounds for labeling regardless of the
predicted objective values [18]. The second strategy
was to calculate the unweighted linear combination
score from the numeric predicted objective values and
choose a batch of compounds with the highest scores
for labeling [3, 12]. At each step of the iterative
process, the labeled and unlabeled objective values
were clustered using K-means with the number of
clusters set to 10 for this case study. Both selection
strategies were repeated for multiple iterations to
produce two time series datasets. Each of these
datasets consisted of 187 chemical descriptors, various
clusterings of chemical descriptors, 5 objective values
for each time point, and single clusterings of the
objective values for each time point.

2.5.3. Time series stability measured a certain level
of uncertainty in predicted objectives and also
identified the interesting clusters and data points.
Uncertainty by this definition is based on inconsistent
clusterings across multiple iterations using the same
clustering algorithm.
2.5.4. Diversity measured the dissimilarity of
compounds based on clustering or chemical properties
to ensure that the entire chemical space was
represented in the simulation [2, 13].
2.5.5. Prediction confidence measured how closely
the predicted values matched labeled values based on
statistics and intuition. This measure is associated
with predictive accuracy [10].
2.5.6
Chemical
association
measured
the
relationships between the objectives and chemical
descriptors.
Visually identifying and possibly
explaining these associations in the dataset can be
used to make predictions about future iterations [10,
15, 18].

3.3. Visualization Results
Parallel coordinates was applied to a selected
compound subset per iteration for both time series
datasets. In figure 2, a subset consisting of 100
compounds is shown based on 5 objectives (O1-O5)
from a single iteration. From the pictures shown, the
subset derived from the random selection strategy
(left) is showing a mixture of cluster memberships
whereas the subset derived from the linear
combination selection strategy (right) strongly favors
cluster 8 (yellow color). This observation indicates a
less diverse selection when using the linear
combination selection strategy. It also indicates that
cluster 8 may be a preferable cluster since the scores
appear to be higher for all 5 objectives. Excluding
objective 3 (O3), the distributions of the objectives
show that the linear combination selection strategy is
converging faster at this particular iteration than the
random selection strategy.
Besides comparing selection strategies, parallel
coordinates was also applied to study distribution
snapshots of labeled and unlabeled compounds across
time. This provided intuitive information related to
prediction confidence. Utilizing these capabilities,
parallel coordinates was found to be effective for
analyzing diversity, convergence, and prediction
confidence and provided the opportunity for further
exploration with complementary tools such as the
heatmap.
As shown in figure 3, the heatmap shows the
relationship of the labeled compound score to various
chemical descriptors for a subset consisting of 100
compounds.

3. Visual Analytics Model Application
3.1. Dataset
We simulated the drug discovery pipeline using a
dataset provided by the Pfizer Corporation for
retrospective analysis of various library design
strategies. The compounds in this dataset were all
labeled with known values from biological testing.
After data cleaning, the dataset consisted of 2824
compounds, 187 chemical descriptors, and 5
objectives to optimize.
The chemical descriptors
were automatically generated using a number of
fingerprinting techniques which capture structural
information and various properties of each compound
[27].

3.2. Simulation
Chemical descriptors were clustered using a
variety of algorithms including density-based
clustering built on K-means (DB), K-means (KM),
expectation maximization (EM), and hierarchical
clustering (HC) using implementations in WEKA [10].
The initialization was randomized and consisted of 7
or 8 compounds chosen from each of 14 clusters using
K-means for a total of 100 starting compounds. The
batch size was set to 100 to satisfy experimental
constraints.

348

Color by KM

O1

O2

O3

O4

O5

O1

O2

O3

O4

O5

Fig.2. Parallel Coordinates shows a labeled subset of 100 compounds based on 5 objectives to
optimize. These subsets were derived from the random selection strategy (left picture) and the
linear combination selection strategy (right picture). The distribution bars on the axes are binned
and colored according to frequency (red for dense, green for sparse).
The compounds are
colored by a single clustering algorithm.
The chemical descriptors were automatically reordered
in reference to the labeled compound score based on
the correlation coefficient calculated using R [5]. The
strongest positively and negatively correlated chemical
descriptors were repositioned closer to the labeled
score column so that chemical associations can be
identified easily. This functionality provided the
ability to visually track these correlations over time.
Including the predicted compound score provided a
means to compare predicted values to labeled values
to indicate prediction confidence.

In figure 4, all 2824 records in the dataset are
shown by using an aggregation function to calculate
the average of 16 records for each row. The level of
aggregation can be interactively adjusted for the entire
dataset or a selected group of records resulting in an
expansion or contraction in the total number of rows in
the graphic. Figure 4 illustrates the relationship of the
K-means clustering result to the chemical descriptors
by reordering the dimensions based on the Euclidean
distance calculated using R.
KM

Score

RF

1 2

3 4

5

6 7

1

2

3

4

5

6 7

8 9 10 11 12 13 14 15 16 17 18 19

C0

8 9 10 11 12 13 14 15 16 17 18

C1

High
Value

High
Value

C2
C3
C4
C5
C6
C7

C8
C9
C10
C11
C12
C13

Low
Value

Low
Value

Fig.4. The heatmap shows the entire dataset
of 2824 compounds.
Each row is an
aggregate of the average value of 16
compounds (high value is red, low value is
blue).
The level of aggregation can be
interactively adjusted for the entire dataset or
a selected group of records. The first column
is sorted based on the categorical K-means
cluster assignment. The 19 remaining
columns are chemical descriptors and show
dominant chemical properties captured by
each cluster.

Fig.3. The heatmap shows a labeled subset of
100 selected compounds.
Each row
represents a single compound (high value is
red, low value is blue). The first column is
sorted based on the labeled compound score
(Score). The second column is the predicted
compound score (RF). The 18 remaining
columns are chemical descriptors and show
positive and negative correlations to the
labeled compound score.

349

The result revealed some dominant chemical
properties captured by each cluster. Using results
previously shown in figure 2, some important
properties of cluster 8 (yellow color) can now be
determined to help understand why the cluster was
favored over the others. This arrangement also
provided insight into inter-cluster and intra-cluster
diversity. The reordering result in figure 4 grouped
together chemical descriptors by similarity. This
presented an opportunity to remove redundant
attributes by visually applying attribute reduction
techniques.
The heatmap extension was shown to be very
versatile and was used to evaluate diversity, prediction
confidence, and chemical association. Presenting a
detailed view of convergence can also be
accomplished by including 5 objective columns in the
graphic [4]. Adding columns with the appropriate
stability metrics enabled analysis of cluster and time
series stability as well. To improve interaction,
parallel coordinates was used as a selection tool to
complement the heatmap. Selections were made by
specifying ranges of values for multiple axes using
Boolean operation functions [21].
C8

C6

C6

of clusters set to 10 for this case study. Stable
compounds are shown as thick smooth bands while the
unstable compounds are shown as thin crossing bands.
The stable compounds were considered members of
strong clusters since the cluster memberships
remained consistent across clustering algorithms. This
type of behavior indicated consensus and the
clustering results were considered more trustworthy
when compared to results from a single clustering
algorithm [28]. Additionally, figure 5 shows cluster
membership changes and cluster splits (e.g. KM-C6
splits into HC-C6 and HC-C5).
As shown in figure 6, the first 3 dimensions are
clusterings of the objective values for the first 3
iterations derived from the random strategy (left) and
the linear combination strategy (right). Additional
dimensions were added to create the split layout. The
record stability dimension (RS) helps to identify stable
and unstable records. The labeled compound score
dimension (Score) relates stability and cluster
membership to the linear combination score for each
of the objectives. This can also be accomplished by
selecting the appropriate records and analyzing them
in a complementary visualization such as the heatmap
or parallel coordinates. Figure 6 demonstrates that the
linear combination selection strategy produced labeled
compounds that were largely unstable and scored
higher when compared to the random selection
strategy. This indicates that an uncertainty based
selection strategy might be appropriate for increasing
performance of the selection model.

C6

C4

C5

4. Conclusion and Future Work
C8

C5

DB

EM

KM

We proposed an integrated visualization and
analysis model to effectively evaluate various
algorithms and compound selection strategies during
the iterative selection process. The visual analytics
model was designed to help understand the current
state of knowledge during a single iteration to enable
decision making and process customization before the
next iteration is executed. By combining visualization
and computational techniques, the visual analytics
model provided a strategy for measuring objective
convergence, cluster stability, time series stability,
diversity, prediction confidence, and chemical
association. Combined with a clustering strategy,
these evaluation metrics provide a framework for
approaching the lead generation library design
problem.
In our future work, we will continue to validate
this model to handle a wider variety of problems
related to drug discovery.

C1

HC

Fig.5. Cluster Comparison and Visualization
(CComViz) shows the entire dataset of 2824
compounds. Each axis is a binned clustering
using
a
particular
algorithm.
Stable
compounds are shown as thick smooth bands
and were selected for further analysis.
Unstable compounds are shown as thin
crossing bands.
In figure 5, CComViz shows the binned
clusterings of 4 clustering algorithms with the number

350

IT1

IT2

IT3

Stable

High

Stable

High

Unstable

Low

Unstable

Low

RS

Score

IT1

IT2

IT3

RS

Score

Fig.6. Cluster Comparison and Visualization (CComViz) shows the entire dataset of 2824
compounds with a highlighted subset of 100 compounds derived from the random selection
strategy (left picture) and the linear combination selection strategy (right picture). Each of the
first 3 dimensions is a binned clustering of a particular iteration (IT1-IT3) using the same
clustering algorithm. The records are colored according to the cluster assignments of the latest
iteration (IT3). Additional dimensions for record stability (RS) and compound score (Score) were
added. These extra dimensions were binned with higher values towards the top and lower values
towards the bottom.
We are also updating our Universal Visualization
Platform to provide the same capabilities in a
collaborative web-based session history serviceoriented architecture.

[3] Trudi Wright, Valerie J. Gillet, Darren V. S. Green, and
Stephen D. Pickett.
“Optimizing the Size and
Configuration of Combinatorial Libraries”, Journal of
Chemical Information and Computer Sciences, 2003,
43 (2), pp 381-390.
[4] Andy Pryke, Sanaz Mostaghim, and Alireza Nazemi.
“Heatmap Visualization of Population Based MultiObjective Algorithms”, Lecture Notes in Computer
Science, 2007, 4403, pp 361-375.
[5] R Development Core Team (2008). R: A Language
and Environment for Statistical Computing.
R
Foundation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0, url:http://www.r-project.org.
[6] Ramana Rao and Stuart K. Card. “The Table Lens:
Merging Graphical and Symbolic Representations in an
Interactive Focus + Context Visualization for Tabular
Information”, Proceedings of SIGCHI Conference on
Human Factors in Computing Systems, ACM, New
York, NY, USA, 1994.
[7] Jianping Zhou, Shawn Konecni, and Georges Grinstein.
“Visually Comparing Multiple Partitions of Data with
Applications to Clustering”, Proceedings SPIE, Vol.
7243, 2009.
[8] Alexander Gee, Hongli Li, Mary Beth Smritic, Urska
Cvek, Howie Goodell, Vivek Gupta, Christine
Lawrence, Jianping Zhou, Chih-Hung Chiang, and
Georges Grinstein. “Universal Visualization Platform”,
Visualization and Data Analysis 2005, Proceedings
SPIE, Vol. 5669, pp 274-283, 2005.

5. Acknowledgements
The authors wish to thank Dr. Peter Henstock,
Statistics & Visualization, Target & Mechanism
Informatics, Pfizer Research Technology Center, and
Dr. Eric Gifford, Pfizer Global Research &
Development, Discovery Research Informatics, for
their support. This project was partially funded by the
Pfizer Corporation.

6. References
[1] Manfred K. Warmuth, Jun Liao, Gunnar Rätsch,
Michael Mathieson, Santosh Putta, and Christian
Lemmen. “Active Learning with Support Vector
Machines in the Drug Discovery Process”. Journal of
Chemical Information and Computer Sciences, 2003,
43 (2), pp 667-673.
[2] Puneet Sharma, Srinivasa Salapaka, and Carolyn Beck.
“Scalable Approach to Combinatorial Library Design
for Drug Discovery”, Journal of Chemical Information
and Modeling, 2008, 48 (1), pp 27-41.

351

[9] Alfred Inselberg and Bernard Dimsdale. “Parallel
Coordinates: A Tool for Visualizing Multi-Dimensional
Geometry”, Proceedings of the 1st Conference on
Visualization, pp 361-378, 1990.
[10] Ian H. Witten and Eibe Frank. Data Mining: Practical
machine learning tools and techniques, 2nd Edition,
Morgan Kaufmann, San Francisco, 2005.
[11] Vladimir Svetnik, Andy Liaw, Christopher Tong, J.
Christopher Culberson, Robert P. Sheridan, and
Bradley P. Feuston. “Random Forest: A Classification
and Regression Tool for Compound Classification and
QSAR Modeling”, Journal of Chemical Information
and Computer Sciences, 2003, 43 (6), pp 1947-1958.
[12] Johan Andersson.
“A Survey of Multiobjective
Optimization in Engineering Design”, Technical
Report, Department of Mechanical Engineering,
Linkoping University, 2000.
[13] Volker Schnecke and Jonas Bostrom. “Computational
Chemistry-Driven Decision Making in Lead
Generation”, Drug Discovery Today, 2006, 11, pp 4350.
[14] Siew Kuen Yeap, Rosalind J. Walley, Mike Snarey,
Willem P. van Hoorn, and Jonathan S. Mason.
“Designing Compound Subsets: Comparison of
Random and Rational Approaches Using Statistical
Simulation”, Journal of Chemical Information and
Modeling, 2007, 47 (6), pp 2149-2158.
[15] Jürgen Bajorath. “Selected Concepts and Investigations
in Compound Classification, Molecular Descriptor
Analysis, and Virtual Screening”, Journal of Chemical
Information and Computer Sciences, 2001, 41 (2), pp
233-245.
[16] Valerie J. Gillet, Wael Khatib, Peter Willett, Peter J.
Fleming, and Darren V. S. Green. “Combinatorial
Library Design Using a Multiobjective Genetic
Algorithm”, Journal of Chemical Information and
Computer Sciences, 2002, 42 (2), pp 375-385.
[17] Dharmesh M. Maniyar, Ian T. Nabney, Bruce S.
Williams, and Andreas Sewing. “Data Visualization
during the Early Stages of Drug Discovery”, Journal of
Chemical Information and Modeling, 2006, 46 (4), pp
1806-1818.
[18] Daniel C. Weaver. “Applying Data Mining Techniques
to Library Design, Lead Generation, and Lead
Optimization”, Current Opinions in Chemical Biology,
2004, 8 (3), pp 264-270.
[19] Alan Dix and Geoffrey Ellis. “By Chance – Enhancing
Interaction with Large Datasets through Statistical
Sampling”, Proceedings Advanced Visual Interfaces,
AVI 2002, Trento, Italy, ACM Press, pp 167-176.
[20] Alfred Inselberg.
“Multidimensional Detective”,
Proceedings 1997 IEEE Symposium on Information
Visualization, pp 100.
[21] Harri Siirtola.
“Direct Manipulation of Parallel
Coordinates”, Conference on Human Factors in
Computing Systems, 2000, pp 119-120.
[22] Gennady Andrienko and Natalia Andrienko. “Blending
Aggregation and Selection: Adapting Parallel
Coordinates for the Visualization of Large Datasets”,
The Cartographic Journal, 2005, 42 (1), pp. 49-60.

[23] Peter Pirolli and Ramana Rao. “Table Lens as a Tool
for Making Sense of Data”, Proceedings of the
Workshop on Advanced Visual Interfaces, 1996, pp 6780.
[24] Georges Grinstein, Marjan Trutschl, and Urska Cvek.
“High Dimensional Visualizations”, Proceedings
Visual Data Mining Workshop, KDD 2001, ACM, New
York.
[25] Liqiang
Geng
and
Howard
J.
Hamilton.
“Interestingness Measures for Data Mining: A Survey”,
ACM Computing Surveys, 38 (9), 2006.
[26] G. B. Mufti, P. Bertrand, and L. E. Moubarki.
“Determining the Number of Groups from Measures of
Cluster Stability”, Proceedings of International
Symposium on Applied Stochastic Models and Data
Analysis, 2005.
[27] J. Bajorath.
“Integration of Virtual and HighThroughput Screening”, Nature Reviews Drug
Discovery, 2002, 1 (11), pp 882-894.
[28] Alexander Strehl and Joydeep Ghosh.
“Cluster
Ensembles – A Knowledge Reuse Framework for
Combining Multiple Partitions”, Journal of Machine
Learning Research, 2002, 3, pp 583-617.

352

