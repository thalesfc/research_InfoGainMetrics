2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

Two Dimensional Compressive Classifier for Sparse Images
1

Armin Eftekhari , Hamid Abrishami Moghaddam1, Massoud Babaie-Zadeh2
1
K.N. Toosi University of Technology, Tehran, Iran
2
Sharif University of Technology, Tehran, Iran
{a.eftekhari@ee.kntu.ac.ir, moghadam@eetd.kntu.ac.ir, mbzadeh@ ee.sharif.edu}
compressive detection for arbitrary signals has been
considered [1, 2]. Extending these results, [2] considered
the problem of multiple hypothesis testing in compressed
domain and [1] proposed a compressive classifier dubbed
smashed filter, which requires knowledge of class
manifolds for successful operation.
However, as shown later in this paper, direct
extension of these results to image domain (2D) is
computationally prohibitive, which strongly hinders the
application of conventional compressive classifier (1DCC) in real-world scenarios. To overcome this major
drawback, the idea of 2D compressive classification is
developed in this paper. First, 2D random projection
scheme is introduced in Section 2 and associated
concentration properties are studied. It is then observed
that Gaussian random matrices, as the most common
choice in 1D compressive framework, are not
appropriate for our 2D random projection scheme. Then,
by adding an assumption of so-called 2D sparsity (in
some basis) to images, desirable concentration properties
are proved for the same set of admissible random
matrices as in 1D framework. This assumption is not
restrictive, as most images become sparse under wellknown transformations, like DCT, or have a sparse edge
map. In Section 3, these findings are exploited to develop
a 2D compressive classifier (2D-CC) for sparse images,
along with derivation of error bound for an important
special case. Finally, 2D-CC is applied to retinal
identification within a realistic setting. It is observed
that, at worst, 2D-CC provides significant saving on
computational load and memory requirements compared
to 1D-CC, at the cost of negligible loss in performance.
This performance loss, however, can be avoided by
â€œwiseâ€ choice of parameters. We note that, to emphasize
on the main result of the article, some intermediate steps
of the derivations have been omitted, and the interested
reader is referred to [5] for details.

Abstract
The theory of compressive sampling involves making
random linear projections of a signal. Provided signal is
sparse in some basis, small number of such
measurements preserves the information in the signal,
with high probability. Following the success in signal
reconstruction, compressive framework has recently
proved useful in classification, particularly hypothesis
testing. In this paper, conventional random projection
scheme is first extended to the image domain and the key
notion of concentration of measure is closely studied.
Findings are then employed to develop a 2D compressive
classifier (2D-CC) for sparse images. Finally,
theoretical results are validated within a realistic
experimental framework.
Keywords--- Compressive sampling,
projections, retinal identification

random

1. Introduction
The recently developed theory of compressive
sampling (CS) involves making random linear
projections of a signal by multiplying a random matrix.
Provided the signal is sparse in some basis, few random
projections preserve the information in the signal, with
high probability [1]. This remarkable result is rooted in
the concentration of measure phenomenon [1], which
implies that the Euclidean length of a vector is uniformly
â€œshrunkâ€ under a variety of random projection matrices,
with high probability. Due to tangible advantages, CS
framework has found many promising applications in
signal and image acquisition, compression, and medical
image processing [1, 2, 4]. The CS community, however,
has mainly focused on the signal reconstruction problem
from random projections to date [3], and other
applications of CS have yet remained unexplored. In
particular, few recent studies showed that classification
can be accurately accomplished using random
projections, which suggests random projections as an
effective and reliable, yet universal feature extraction
and dimension reduction tool. In this context,
978-0-7695-3789-4/09 $25.00 Â© 2009 IEEE
DOI 10.1109/CGIV.2009.68

2. Concentration of Measure for 2D Random
Projection Scheme
In order to linearly project a given image  â€«×â€¬
Ô¹à¯¡à°­àµˆà¯¡à°® to lower dimensional space, columns of  are
conventionally stacked into a vector Âš àµŒ Â˜Â‡Â…áˆºáˆ». This
402

corresponding to the indices in Ü´à¯œ . Obviously, rows of
à¯‹à³” are independent. Now, we may write Ô¡ÂšÔ¡à¬¶à¬¶ àµŒ
à¯¤
Ïƒà¯œà­€à¬µÔ¡à¯‹à³” ÂšÔ¡à¬¶à¬¶ , where entries of à¯‹à³” are zero-mean rvâ€™s
with variance Í³È€İŠà¬µ İŠà¬¶ . It is then straightforward to show
that strong concentration of à¯‹à³” for İ… àµŒ Í³Ç¡ Ç¥ Ç¡ â€«İâ€¬, implies
that of  [5]. This follows easily from the following fact:
İ‰à¬µ İ‰à¬¶
İ‰à¬µ İ‰à¬¶
Ô¡ÂšÔ¡à¬¶à¬¶ à¸¬ àµ’
Â” àµ¤à¸¬Ô¡ÂšÔ¡à¬¶à¬¶ àµ†
Ô–àµ¨ àµ‘
İŠà¬µ İŠà¬¶
İŠà¬µ İŠà¬¶
à¯¤
(2)
ÈÜ´à¯œ È
ÈÜ´à¯œ È
Ô¡ÂšÔ¡à¬¶à¬¶ á‰¤ àµ’
à· Â” á‰ˆá‰¤Ô¡à¯‹à³” ÂšÔ¡à¬¶à¬¶ àµ†
ß³á‰‰
İŠà¬µ İŠà¬¶
İŠà¬µ İŠà¬¶

process, however, ignores the intrinsic row/column-wise
structure of the image and, even for moderately sized
matrices, involves prohibitive computational load and
memory requirements for generation and manipulation of
the projection matrix. As a remedy to these drawbacks,
one may use so-called 2D projection scheme, i.e.
 àµŒ  à­˜ , in which  â€« ×â€¬Ô¹à¯ à°­ àµˆà¯ à°® ,  â€« ×â€¬Ô¹à¯ à°­àµˆà¯¡à°­ ,
 â€« ×â€¬Ô¹à¯ à°® àµˆà¯¡à°® , with İ‰à¬µ àµ İŠà¬µ and İ‰à¬¶ àµ İŠà¬¶ . Note that, in
terms of storage requirements, 2D projection scheme
requires only İ‰à¬µ İŠà¬µ àµ… İ‰à¬¶ İŠà¬¶ memory units for projection
matrices, whereas conventional projection scheme to
Ô¹à¯ à°­ à¯ à°® requires İ‰à¬µ İ‰à¬¶ İŠà¬µ İŠà¬¶ memory units. Similarly,
using a naive matrix multiplication procedure,
computational complexity of 2D projection scheme is an
order less than conventional projection scheme to
Ô¹à¯ à°­ à¯ à°® . In this section, properties of 2D projection
scheme in the compressive framework are studied.
Clearly, successful inference in this scenario
depends on preservation of the structure of samples after
projection [1]. For 1D signals, this has received
extensive treatment. Particularly, it has been shown that,
given a 1D signal, random matrices whose entries are
i.i.d. random variables (rvâ€™s) with proper tail bounds, e.g.
Gaussian rv, uniformly â€œshrinkâ€ the signal length after
projection, with high probability. Using the union bound,
one may then show that, with high probability, such
random matrices preserve the structure of a set of
samples after projection, by uniformly shrinking the pairwise Euclidean distances. Focusing on the Gaussian
random matrices, as the most common choice in CS
framework, a similar strategy is developed in the
following. Consider random Gaussian matrices  â€«×â€¬
Ô¹à¯ à°­ àµˆà¯¡à°­ and  â€« ×â€¬Ô¹à¯ à°® àµˆà¯¡à°® , whose entries are i.i.d.
à£¨áˆºÍ²Ç¡Í³È€İŠà¬µ áˆ» and à£¨áˆºÍ²Ç¡Í³È€İŠà¬¶ áˆ» rvâ€™s, respectively. Given
 â€« ×â€¬Ô¹à¯¡à°­àµˆà¯¡à°® , let us define Âš àµŒ Â˜Â‡Â…áˆºáˆ» and  àµŒ Ûª,
where Ûª denotes the Kronecker product. Thus,  à­˜ àµŒ
Âš, where each entry of  has a Bessel-like distribution
with zero mean and variance Í³È€İŠà¬µ İŠà¬¶ [5]. We are
interested in studying the concentration of rv Ô¡ÂšÔ¡à¬¶à¬¶
about its expected value áˆºİ‰à¬µ İ‰à¬¶ Î¤İŠà¬µ İŠà¬¶ áˆ»Ô¡ÂšÔ¡à¬¶à¬¶ by finding
an exponentially-fast decreasing bound on:
İ‰à¬µ İ‰à¬¶
İ‰à¬µ İ‰à¬¶
Ô¡ÂšÔ¡à¬¶à¬¶ à¸¬ àµ’
Â” àµ¤à¸¬Ô¡ÂšÔ¡à¬¶à¬¶ àµ†
ß³àµ¨
İŠà¬µ İŠà¬¶
İŠà¬µ İŠà¬¶
(1)

à¯œà­€à¬µ

For ease of notation, let  àµŒ à¯‹à³” and Ü´ àµŒ ÈÜ´à¯œ È.
Now, it suffices to find an exponentially-fast decreasing
bound for Â”áˆ¼ÈÔ¡ÂšÔ¡à¬¶à¬¶ àµ† Ü´Î¤İŠà¬µ È àµ’ Ü´ß³ Î¤İŠà¬µ İŠà¬¶ áˆ½, where, due
to linearity, each column of  is assumed to have unit
length. Consider the probability of Ô¡ÂšÔ¡à¬¶à¬¶ àµ† Ü´Î¤İŠà¬µ àµ’
Ü´ß³ Î¤İŠà¬µ İŠà¬¶ . Then, for any İ„ àµ Í², invoking the Chernoff
bounding technique and using the inherent symmetries in
 [5] gives:
Â” àµ¤Ô¡ÂšÔ¡à¬¶à¬¶ àµ†
àµ‘ à¥± á‰ˆİ

Ü´
Ü´
àµ’
ß³àµ¨
İŠà¬µ İŠà¬µ İŠà¬¶
à°® à¯‹

à³™ à³™

à°­ à°®à¯† à¯« á‰
à¯›á‰€Ïƒà³•à°¸à°­
à°­Ç¡à³• à³•

á‰‰ İ

à¯‹
à¬¿
à¯›áˆºà¯¡à°® à¬¾à°¢áˆ»
à¯¡à°­ à¯¡à°®

(3)

where, without loss of generality, first row of  is used
in (3). Further simplifications and exploiting the
moments of Gaussian and Chi-square distributions [5]
leads to:
à¥± á‰ˆİ

à³™ à³™

à°®

à°­ à°®à¯† à¯« á‰
à¯›á‰€Ïƒà³•à°¸à°­
à°­Ç¡à³• à³•

á‰‰

İŠà¬µ
(4)
Í´İ„ à¯ Í´İ‡Ç¨ Ä« á‰€İ‡ àµ… Í´ á‰
àµ°

İŠ
İŠà¬µ İŠà¬¶ áˆºİ‡Ç¨áˆ»à¬¶
Ä« á‰€ à¬µá‰
à¯à­€à¬´
Í´
in which Ä«áˆºÈ‰áˆ» denotes the Gamma function. The sum on
the right hand side of (4) apparently fails to converge.
Consequently, the probability of Ô¡ÂšÔ¡à¬¶à¬¶ àµ† Ü´Î¤İŠà¬µ àµ’
Ü´ß³ Î¤İŠà¬µ İŠà¬¶ fails to decrease exponentially fast, in general;
not letting us to extend the random projection scheme to
image domain for random Gaussian matrices. In the
following, we prefer to insert an additional constraint on
, which would allow for successful operation of 2D
random projection scheme with the same set of
admissible random matrices as in 1D case. Let È­à¯ denote
the set of all signals of length Â with at most Â nonzero
entries. We say that  â€« ×â€¬Ô¹à¯ àµˆà¯¡ satisfies restricted
isometry property (RIP) of order İ‡, if for every Â˜ â€« ×â€¬È­à¯ ,
the following holds for some ß³à¯ â€« ×â€¬áˆ¾Í²Ç¡Í³áˆ».
İ‰
İ‰
áˆºÍ³ àµ† ß³à¯ áˆ» Ô¡Â˜Ô¡à¬¶à¬¶ àµ‘ Ô¡Â˜Ô¡à¬¶à¬¶ àµ‘ Ô¡Â˜Ô¡à¬¶à¬¶ áˆºÍ³ àµ… ß³à¯ áˆ»
(5)
İŠ
İŠ
à®¶

àµ‘ à·àµ¬

where probability is taken over all matrices  and .
Note that (in contrast to 1D counterpart), entries of  are
no more independently distributed. Furthermore, each
entry of  is a product of two i.i.d. Gaussian rvâ€™s.
Adopting the idea presented in [4], let us construct an
undirected graph â€« Ü©â€¬àµŒ áˆºÜ¸Ç¡ â€«Ü§â€¬áˆ» with Ü¸ àµŒ áˆ¼Í³Ç¡ Ç¥ Ç¡ İ‰à¬µ İ‰à¬¶ áˆ½,
where any two pair of vertices are connected by an edge
iff corresponding rows in  are dependent. Clearly, each
row of  is dependent with exactly â€« İâ€¬àµ† Í³ àµŒ İ‰à¬µ àµ… İ‰à¬¶ àµ†
Í´ other rows in . Thus, maximum degree of â€« Ü©â€¬is â€« İâ€¬àµ† Í³,
and we can partition â€« Ü©â€¬into â€«( İâ€¬or more) nonoverlapping
à¯¤
partitions áˆ¼Ü´à¯œ áˆ½à¯œà­€à¬µ , such that vertices in each partition do
not share any edges [4, 12]. Next, let à¯‹à³” be the ÈÜ´à¯œ È àµˆ
İŠà¬µ İŠà¬¶ submatrix obtained by retaining the rows of 

Using arguments on tail bounds, it is shown that
many random matrices satisfy RIP condition with high
probability [1, 4]. For instance, İ‰ àµˆ İŠ matrices whose
entries are i.i.d. à£¨áˆºÍ²Ç¡Í³È€İŠáˆ» rvâ€™s satisfy RIP of order İ‡
with probability exceeding Í³ àµ† Í´İ à¬¿à¯–à°®áˆºà°¢à³–áˆ»à¯  , provided
İ‡ àµ‘ Ü¿à¬µ áˆºß³à¯ áˆ»İ‰ ÂÂ‘Â‰áˆºİŠÎ¤İ‡ áˆ», where Ü¿à¬µ áˆºß³à¯ áˆ» and Ü¿à¬¶ áˆºß³à¯ áˆ» depend
403

only on ß³à¯ [1, 4]. Also random orthoprojectors, i.e.
matrices with random orthonormal rows, satisfy the RIP
condition similarly. Extending these ideas to image
domain, let È­à¯à³ Ç¡à¯à³ denote the set of all images  â€«×â€¬
Ô¹à¯¡à°­àµˆà¯¡à°® , whose nonzero entries are distributed in at most
İ‡à¯¥ rows and İ‡à¯– columns. A matrix with this property will
be called 2D sparse matrix. Now, we can make the
following observation [5].
Observation 1. Suppose projection matrices  â€«×â€¬
Ô¹à¯ à°­ àµˆà¯¡à°­ and  â€« ×â€¬Ô¹à¯ à°® àµˆà¯¡à°® , respectively, satisfy the RIP
conditions of orders Í´İ‡à¯¥ and Í´İ‡à¯– , for some ß³à¬¶à¯à³ Ç¡ ß³à¬¶à¯à³ â€«×â€¬
áˆ¾Í²Ç¡Í³áˆ». Then, for any  â€« ×â€¬È­à¬¶à¯à³ Ç¡à¬¶à¯à³ , we have:
İ‰à¬µ İ‰à¬¶
Ô¡Ô¡à¬¶à¬¶ àµ‘ Ô¡ à­˜ Ô¡à¬¶à¬¶ 
İŠà¬µ İŠà¬¶
İ‰à¬µ İ‰à¬¶
àµ«Í³ àµ… ß³à¬¶à¯à³ àµ¯àµ«Í³ àµ… ß³à¬¶à¯à³ àµ¯
àµ‘ Ô¡Ô¡à¬¶à¬¶
İŠà¬µ İŠà¬¶

Í³
Â”Â”áˆºÇ¡ áˆ» àµŒ Í³ àµ† à¶± ÂÂƒÂšáˆ¼â€«İŒâ€¬à¯Ÿ áˆºÂ›áˆ»áˆ½ Â†Â›
â€« à­· Ü®â€¬à¯Ÿ

where
â€«İŒâ€¬à¯Ÿ áˆºÂ›áˆ» àµŒ à£¨àµ«Âšà¯Ÿ Ç¡ ßª à¬¶ à¯¡à°­à¯¡à°® àµ¯ is the conditional
distribution of Â› given Âšà¯Ÿ . Note that, for any nonnegative
sets áˆ¼Ü½à¯Ÿ áˆ½ and áˆ¼â€«İâ€¬à¯Ÿ áˆ½ with Ïƒà¯…à¯Ÿà­€à¬µ â€«İâ€¬à¯Ÿ àµŒ Í³, we have ÂÂƒÂšà¯Ÿ áˆ¼Ü½à¯Ÿ áˆ½ àµ’
Ï‚à¯…à¯Ÿà­€à¬µ Ü½à¯Ÿà¯¦à³— . Consequently, we may write:
à¯…

Â”Â”áˆºÇ¡ áˆ» àµ‘ Í³ àµ†

Â”Â”áˆºÇ¡ áˆ» àµ‘ Í³ àµ† Â‡ÂšÂ’ á‰†àµ†

Ïƒà¯Ÿà®·à¯Ÿá‡² Ô¡áˆºÂšà¯Ÿ àµ† Âšà¯Ÿá‡² áˆ»Ô¡à¬¶à¬¶
á‰‡
Í¶ßª à¬¶ â€«Ü®â€¬à¬¶

(10)

Assuming that  and  satisfy the RIP conditions similar
à¬¶
â€«ÂÂ‹Â Øœâ€¬à¯Ÿà®·à¯Ÿá‡² Ô¡Âšà¯Ÿ àµ†
to Observation 1, and defining İ€à¯ à¯œà¯¡
à¬¶
Âšà¯Ÿá‡² Ô¡à¬¶ , Observation 1 implies the following bound for
classification error [5]:

(6)

Í³ àµ† Â‡ÂšÂ’ á‰†àµ†àµ«Í³ àµ… ß³à¬¶à¯à³ àµ¯àµ«Í³ àµ… ß³à¬¶à¯à³ àµ¯

İ‰à¬µ İ‰à¬¶ áˆºâ€« Ü®â€¬àµ† Í³áˆ» à¬¶
İ€ á‰‡
İŠà¬µ İŠà¬¶ Íºßª à¬¶ â€« Ü®â€¬à¯ à¯œà¯¡
(11)

Particularly, if  and  are random orthoprojectors, then
above bound holds with conditions noted right after
Observation 1. It is observed that the classification error
decays exponentially fast as the number of random
observations İ‰à¬µ İ‰à¬¶ increases.

3. Two Dimensional Compressive Classifier
for Sparse Images

4. Experiments

In many applications, images are either sparse in the
pixel domain or have a sparse representation in some
basis, such that their nonzero entries are concentrated in
a small number of rows/columns. Examples include a
images with sparse edge map or DCT transform of
natural images [10]. For this class of images, obtained
results in previous section are applicable; enabling us to
develop 2D-CC. Formally, assume that à£² àµŒ áˆ¼à¬µ Ç¡ Ç¥ Ç¡ à¯… áˆ½
denotes a set of İŠà¬µ àµˆ İŠà¬¶ known 2D sparse images, i.e.
à¯Ÿ â€« ×â€¬È­à¯à³ Ç¡à¯à³ , İˆ àµŒ Í³Ç¡ Ç¥ Ç¡ â€«Ü®â€¬, for some integers İ‡à¯¥ àµ İŠà¬µ ,
İ‡à¯– àµ İŠà¬¶ . The (possibly noisy) â€œtrueâ€ image  à¯ â€«à£² ×â€¬
undergoes 2D random projection to obtain  àµŒ áˆº à¯ àµ…
áˆ»à­˜ , where  â€« ×â€¬Ô¹à¯¡à°­àµˆà¯¡à°® represents the noise. Now, we
will be concerned with discrimination among the
members of à£², given only low-dimensional random
projectionsÇ¤ Given  and , failure will be quantified in
terms of expected error. Let Â› àµŒ Â˜Â‡Â…áˆºáˆ», Âš àµŒ Â˜Â‡Â…áˆºáˆ»,
Â àµŒ Â˜Â‡Â…áˆºáˆ» and  àµŒ Ûª. For simplicity of analysis,
we further assume ÂÌ±à£¨àµ«Í²Ç¡ ßª à¬¶ à¯¡à°­à¯¡à°® àµ¯, where à¯” denotes
the Ü½ àµˆ Ü½ identity matrix. Also,  and  are selected to
be random orthoprojectors, which implies that the
distribution of noise remains unchanged under
projection. Now, provided à¯Ÿ â€™s happen equally likely, the
Bayes decision rule and the associated expected error
would be [11]:
à­œà³— â€«à£²×â€¬

(9)

Further simplification and setting â€«İâ€¬à¯Ÿ àµŒ Í³È€â€«Ü®â€¬, gives the
following bound [5]:

In particular, if  and  are admissible random
matrices in the 1D compressive framework, (6) holds
with
probability
exceeding
Í³ àµ† Í´İ à¬¿à¯–à°®áˆºà°¢à°®à³–à³ áˆ»à¯ à°­ àµ†
à¬¿à¯–à°® áˆºà°¢à°®à³–à³ áˆ»à¯ à°®
, provided Í´İ‡à¯¥ àµ‘ Ü¿à¬µ áˆºß³à¬¶à¯à³ áˆ»İ‰à¬µ ÂÂ‘Â‰áˆºİŠà¬µ Î¤Í´İ‡à¯¥ áˆ»
Í´İ
and Í´İ‡à¯– àµ‘ Ü¿à¬µ áˆºß³à¬¶à¯à³ áˆ»İ‰à¬¶ ÂÂ‘Â‰áˆºİŠà¬¶ Î¤Í´İ‡à¯– áˆ». This means that, if
 â€« ×â€¬È­à¬¶à¯à³ Ç¡à¬¶à¯à³ , then Ô¡ à­˜ Ô¡à¬¶à¬¶ È€Ô¡Ô¡à¬¶à¬¶ is strongly
concentrated about its expected value for variety of
random matrices; hence establishing concentration of
measure in 2D random projection scheme.

à­¶à³— â€«à­´×â€¬à­£à­¡áˆºà£²áˆ»

Í³
à¯¦à³—
à¶± à·‘àµ«â€«İŒâ€¬à¯Ÿ áˆºÂ›áˆ»àµ¯ Â†Â›
â€«à­· Ü®â€¬
à¯Ÿà­€à¬µ

àµ«Í³ àµ† ß³à¬¶à¯à³ àµ¯àµ«Í³ àµ† ß³à¬¶à¯à³ àµ¯

Âšà·œà¯Ÿ àµŒ ÂƒÂ”Â‰ÂÂ‹Â Ô¡Â› àµ† Âšà¯Ÿ Ô¡à¬¶ àµŒ ÂƒÂ”Â‰ÂÂ‹ÂÔ¡ àµ† à¯Ÿ à­˜ Ô¡à¬¶

(8)

In this section, the efficacy of the proposed 2D-CC
is examined in retinal identification problem. Retinal
biometrics refers to identity verification of individuals
based on their retinal images. Salient anatomical features
of retina are depicted in Figure 1.a, among which vessel
tree pattern is a superior biometric trait, as it is unique,
time invariant and almost impossible to forge. Our
experiments are conducted on VARIA database
containing 153 (multiple) retinal images of 59
individuals [7]. To compensate for the variations in the
location of optic disc (OD) in retinal images and to
exploit the larger diameter of vessels near OD, a circular
region of interest (ROI) in the vicinity of OD is used to
construct the feature matrix. To extract the ROI, center
of OD is first localized using template matching as in [6],
followed by simple analysis of area. Then, similar to [6],
vessel tree is extracted by a combination of local contrast
enhancement and histogram thresholding. Then, a ringshaped mask with proper radii centered at OD is used to
form the feature matrix  â€« ×â€¬Ô¹à¯¡à°­àµˆà¯¡à°® by collecting the
pixels along İŠà¬¶ àµŒ Í´Í²Í² beams of length İŠà¬µ àµŒ Í³Í²Í²
originating from OD (Figure 1.c). Note that feature
matrices readily satisfy the requirements of Observation
1 in pixel domain, as İ‡à¯¥ àµ İŠà¬µ and İ‡à¯– â€«İŠ Ø§â€¬à¬¶ . Due to small
number of images per subject (~2) and approximate
invariance of feature matrix to the location of OD,
feature matrices of the same subject are modeled as noisy

(7)

404

deviations from corresponding mean feature matrix.
Hence, once all images are processed, à£² àµŒ áˆ¼à¬µ Ç¡ Ç¥ Ç¡ à¯… áˆ½ is
formed, where each à¯Ÿ is the mean feature matrix of İˆth
subject. Dimension reduction and classification of a test
feature matrix is then performed in Ô¹à¯ à°­àµˆà¯ à°® by the
proposed 2D-CC, and in Ô¹à¯ à°­ à¯ à°® by 1D-CC. Error is
measured using the leave-one-out scheme and the
average results over 100 independent repetitions are
depicted in Figure 2 for a wide range of İ‰à¬µ and İ‰à¬¶ .
Although explicit calculation of the bound in (11) is
intractable [1], we note that the exponential nature of
error is in accordance with our findings. Also, due to
highly redundant nature of feature matrices along their
columns, â€œwiseâ€ choices for İ‰à¬µ and İ‰à¬¶ , which consider
this redundancy, exhibit good performance especially for
small values of İ‰à¬µ and İ‰à¬¶ . In contrast, â€œcarelessâ€
choices for İ‰à¬µ and İ‰à¬¶ , degrade the performance (Figure
3). Using an Intel Core 2 Duo, 2.67 GHz processor with
3.24 GB of memory, we found that each repetition of
2D-CC approximately took Í²Ç¤Í²Í²Í»Íºİ‰à¬µ İ‰à¬¶ seconds,
whereas this number was roughly Í²Ç¤Í¸Í·Í·İ‰à¬µ İ‰à¬¶ for 1DCC, in MATLAB7 environment. Note that this
difference is significant even with our small-sized feature
matrices. In sum, for typical choices of İ‰à¬µ and İ‰à¬¶ , 2DCC runs much faster than 1D-CC, yet producing results
with negligible loss in performance. This loss, however,
disappears with proper choice for İ‰à¬µ and İ‰à¬¶ which
takes the prior knowledge into account. In addition, 2DCC enjoys significantly less memory requirements.

(a)

(b)

(c)
Figure 1. (a) Retinal image; bright area is OD. (b) Vessel tree
(in white) and mask (in blue). (c) Feature matrix for İŠà¬µ àµŒ Í³Í²Í²,
İŠà¬¶ àµŒ ÍµÍ²Í² (images (a) and (b) are cropped).

Figure 2. Average classification error of 2D-CC (red surface)
and 1D-CC (blue surface) for a wide range of İ‰à¬µ and İ‰à¬¶ .

Conclusions
In this paper, the idea of random projections is
extended to image domain and associated concentration
properties are studied. Our findings are then used to
develop 2D-CC, along with error bound for an important
special case. Finally, results are validated in a real-world
application.

Figure 3. Two examples of â€œwiseâ€ choices which consider the
redundancy along columns: İ‰à¬µ àµŒ Í³ (left) and İ‰à¬µ àµŒ Íµ (right)

References

[6] H. Farzin, H. Abrishami, â€œA novel retinal identification
system,â€ EURASIP Jr. on Advances in Signal Proc., 2008
[7] VARIA
database,
available
online
at
http://www.varpa.es/varia.html
[8] A. Ghaffari, M. Babaie-Zadeh, C. Jutten, â€œSparse
decomposition of two dimensional signals,â€ Int. Conf.
Acoustics, Speech, and Signal Proc., (ICASSPâ€™09),
Accepted.
[9] K.B. Peterson, M.S. Pederson, â€œMatrix Cookbook,â€
Available online at http://matrixcookbook.com
[10] J.-L. Starck, M. Elad, D.L. Donoho, â€œImage
decomposition via the combination of sparse
representations and a variational approach,â€ IEEE
Transactions on Image Processing, 2006.
[11] R.O. Duda, P.E. Hart, D.G. Stork, â€œPattern Classification,â€
John Wiley, 2nd edition, 2001.
[12] A. Hajnal, and E. Szemeredi, â€œProof of a Conjecture of P.
Erdos,â€ in Combinatorial Theory and its Application, P.
Erdos, A. RÂ´enyi, and V. T. SÂ´os, Eds., pp. 601â€“623.
North-Holland, Amsterdam, 1970.

[1] M.A. Davenport, M.F. Duarte, M.B. Wakin, J.N. Laska,
â€œThe smashed filter for compressive classification and
target recognition,â€ Proc. SPIE Computational Imaging V,
2007.
[2] J. Haupt, R. Castro, R. Nowak, G. Fudge, A. Yeh,
â€œCompressive sampling for signal classification,â€ Fortieth
Asilomar Conf. on Signals, Systems and Computers, 2006.
[3] A. Eftekhari, M. Babaie-Zadeh, C. Jutten, H. Abrishami
Moghaddam, â€œRobust-SL0 for stable sparse representation
in noisy settings,â€ IEEE International Conference on
Acoustics, Speech, and Signal Processing, (ICASSPâ€™09),
Accepted.
[4] J. Haupt, W.U. Bajwa, G. Raz, â€œToeplitz compressed
sensing matrices with applications to channel sensing,â€
Submitted, 2008.
[5] A. Eftekhari, H. Abrishami Moghaddam, M. BabaieZadeh, â€œTwo dimensional compressive classifier for
sparse images,â€ Technical Report, available online at:
http://www.ee.kntu.ac.ir/pow/detlbiog.asp?bioguser=mogh
adam&bioglastn=Abrishami Moghaddam&group=*

405

