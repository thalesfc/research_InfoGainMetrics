2011 Eighth International Conference Computer Graphics, Imaging and Visualization

Local Fisher discriminant analysis with maximum
margin criterion for image recognition
Hong Huang, Jiamin Liu and Yinsong Pan
Key Lab. on Opto-electronic Technique and Systems, Ministry of Education, Chongqing University,
400044/Chongqing, China
Email: {hhuang, liujm, panys}@cqu.edu.cn
Such methods include isometric mapping (Isomap)[6], locally
linear embedding (LLE)[7], laplacian eigenmaps[8], and local
tangent space alignment (LTSA)[9] etc. While these methods
are defined only on training data, and the issue of how to
map new test data remains difficult. Therefore, they cannot be
applied directly to recognition problems.
Recently, some methods resolve this difficulty by finding a
mapping on the whole data space, rather than on training data;
for instance, locality preserving projections (LPP)[10], [11]
and neighborhood preserving embedding (NPE)[12]. These
methods are designed to preserve data locality or similarity in the embedding space rather than good discrimination
capability. Some manifold learning algorithms explicitly address classification problems, such as marginal Fisher analysis (MFA)[13], and locality sensitive discriminant analysis
(LSDA)[14]. The above methods tend to give undesirable
results if samples in a class form several separate clusters
(i.e., multimodal). To address this shortcoming, Sugiyama
proposed a new dimensionality reduction method called local
Fisher discriminant analysis (LFDA)[15], which effectively
combines the ideas of FDA and LPP; that is, LFDA maximizes
the between-class separability and preserves the within-class
local structure at the same time. Thus, LFDA is useful for
dimensionality reduction of multimodal labeled data.
However, LFDA is considered a local discriminant method
for manifold learning, since the discriminative margin in
LFDA is determined by the limited nearby data pairs, while
LDA and MMC explore global information of different classes
in all samples for discrimination. As shown in [16-18], local
discriminant methods(i.e. MFA, LSDA, LFDA) may be optimal for a part of dataset and lose counterpoise for supervised
learning. In some cases, the performance of LFDA may
be much worse than that of LDA and MMC, and fails to
dimensionality reduction and classification.
In order to overcome the unbalanced learning problem in
LFDA for image recognition, we propose a new method, called
local and global marginal discriminant analysis (LGMDA).
This method combines global discriminant measure and local
clustering information together to achieve good discrimination
for better classification. The global discriminant measure enforces separation of overlapping samples from different classes
and local discriminant efforts compact data in the same class.
The proposed method is to eliminate or reduce local structures
with large between-class overlaps via the linear projection.

Abstractâ€”Reducing the dimensionality of data without losing
intrinsic information is an important preprocessing step in image
recognition. Local Fisher Discriminant Analysis (LFDA) is a
linear projective map that arises by solving the multimodal
problem, which effectively combines the ideas of FDA and
LPP. However, since the limited data pairs are employed to
determine the discriminative ability, such local discriminative
methods usually suffer from the maladjusted learning. To improve the discriminant ability of LFDA, this paper proposed
an improved manifold learning method, called local and global
marginal discriminant analysis (LGMDA), by incorporating the
maximum margin criterion (MMC) for image recognition. As a
result, the proposed method tries to find the submanifold that
best discriminates different classes and preserves the intrinsic
relations of the local neighborhood in the same class according
to prior class information. Experiments on the COIL-20 and
YaleB images databases show the effectiveness of the proposed
LGMDA.
Index Termsâ€”face recognition, dimensionality reduction, local
Fisher discriminant analysis, maximum margin criterion, local
and global Marginal discriminant analysis.

I. I NTRODUCTION
Image recognition is a topic of considerable interest in
pattern recognition and computer vision. Over the past few
years, appearance-based methods have become more and more
popular, and are used to deal with image recognition tasks.
However, in many practical image recognition applications, the
performance of these methods may be degraded when faced
with many features that are not necessary for classification[1],
[2]. An important question in pattern recognition is how to
extract a small number of good features[3], [4].
One common way to overcome or alleviate this problem
is by using subspace learning to reduce the feature dimension, and many such subspace learning algorithms have been
proposed in the literature. Representative subspace learning
algorithms include principal component analysis (PCA), Fisher
discriminant analysis (FDA) and maximum margin criterion
(MMC)[5], which assume that the samples lie on a linear
embedded subspace and aim at preserving the global Euclidean structure of the image space. However, research has
shown that facial images lie on or near a smooth nonlinear
manifold. PCA and FDA for dimensionality reduction, often
fail to discover the intrinsic manifold structure of the image
space[4], [6]. Manifold learning method is a perfect tool for
data mining that discovers the structure of high dimensional
data sets and provides better understanding of data[6], [7].
978-0-7695-4484-7/11 $26.00 Â© 2011 IEEE
DOI 10.1109/CGIV.2011.28

92

The effectiveness of the proposed algorithm is verified by
experiments on two well-known image databases.
This paper is organized as follows. In Section 2, we
first present a brief review of dimensionality reduction algorithms, and discuss the limitations of existing algorithms. The
LGMDA method is introduced in Section 3. Section 4 presents
the experimental results to demonstrate the effectiveness of
LGMDA. Finally, we provide some concluding remarks and
suggestions for future work in Section 5.

C. LGMDA
To improve the discriminant ability of LFDA, we incorporate the idea of iMMC to combine global discriminant
measure and local clustering information together to achieve
good discrimination for image recognition.
At first, we make an assumption that naturally occurring
data may be generated by structured systems with possibly
much fewer degrees of freedom than what the ambient dimension would suggest. Thus, we consider the case when the
data lives on or close to a manifold â„³ of the ambient space.
In order to model the local geometrical structure of â„³, we
first construct a nearest neighbor graph G. For each data point
xğ‘– , we find its k nearest neighbors and put an edge between xğ‘–
and its neighbors. Let ğ‘ (xğ‘– ) = {x1ğ‘– , x2ğ‘– , . . . , xğ‘˜ğ‘– } be the set of
its k nearest neighbors. Thus, the weight matrix of G can be
defined as the Heat Kernel. However, it is not easy to choose
an appropriate parameter ğœ for the Heat Kernel. And also, the
Heat Kernel gives a non-sparse affinity matrix. It would be
computationally advantageous if the affinity matrix is sparse.
A sparse affinity matrix can be obtained by assigning positive
affinity values only to neighboring samples, so we define the
weight matrix of G as follows
{
â€²
ğ‘–ğ‘“ xğ‘– âˆˆ N(xğ‘— ) or xğ‘— âˆˆ N(xğ‘– )
1
(3)
Ağ‘–ğ‘— =
0
ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’

II. T HE PROPOSED LGMDA METHOD
A. Typical Behavior of LFDA and MMC
Dimensionality reduction results obtained by the above two
methods are illustrated in Fig. 1 â€“ two-dimensional two-class
data samples are embedded into a one-dimensional space.
For the simple dataset depicted in Fig. 1(a), both LFDA and
MMC nicely separate the samples in different classes from
each other. As illustrated in Fig. 1(b), LFDA can overcome
the intrinsic limitation of MMC that learns the projection
direction to nicely separate the labeled samples. However, the
performance of LFDA is much worse than that of MMC as
shown in Fig. 1(c), which may be caused by the discriminative
margin in LFDA is determined by the limited nearby data
pairs, so it may be optimal for a part of dataset and lose
counterpoise for all samples. It implies that either LFDA
or MMC has a particular advantage over the other one that
motivates us to improve the discriminant ability of LFDA.

â€²

The nearest neighbor graph G with weight matrix A characterizes the local geometry of the data manifold. However,
this graph fails to discover the discriminant structure in the
data. In order to discover both geometrical and discriminant
structures of the data manifold, we construct two graphs, that
is, within-manifold graph Gğ‘¤ and between-manifold graph
Gğ‘ . For each data point xğ‘– , the set ğ‘ (xğ‘– ) can naturally be
split into two subsets: ğ‘ğ‘ (xğ‘– ) and ğ‘ğ‘¤ (xğ‘– ). ğ‘ğ‘ (xğ‘– ) contains
the neighbors having different class labels, and ğ‘ğ‘¤ (xğ‘– ) contains the neighbors sharing the same label with xğ‘– . Clearly,
ğ‘ğ‘ (xğ‘– ) âˆ© ğ‘ğ‘¤ (xğ‘– ) = âˆ… and ğ‘ğ‘ (xğ‘– ) âˆª ğ‘ğ‘¤ (xğ‘– ) = ğ‘ (xğ‘– ).
â€²
Let Wğ‘™ğ‘ be the weight matrix of Gğ‘ and Wâ€²ğ‘™ğ‘Š be the weight
matrix of Gğ‘¤ . We define the following:
{ â€²
â€²
ğ´ğ‘–,ğ‘— (1/ğ‘› âˆ’ 1/ğ‘›â„“ğ‘– ) ğ‘–ğ‘“ â„“ğ‘– = â„“ğ‘—
(4)
Wğ‘™ğ‘,ğ‘–ğ‘— =
1/ğ‘›
ğ‘–ğ‘“ â„“ğ‘– âˆ•= â„“ğ‘—
{ â€²
â€²
ğ‘–ğ‘“ â„“ğ‘– = â„“ğ‘—
ğ´ğ‘–,ğ‘— /ğ‘›â„“ğ‘–
(5)
Wğ‘™ğ‘¤,ğ‘–ğ‘— =
0
ğ‘–ğ‘“ â„“ğ‘– âˆ•= â„“ğ‘—

B. Motivation
To improve the classification performance of LFDA, the
maximum margin criterion is employed to supply a gap for the
maladjusted marginal learning. Though information of classes
is well considered, MMC has a shortage in the definition of
between-class scatter matrices. The sample mean of total data
and various types are separated as possible. However, owning
to the effect of edge class, some classes may become close to
each other, which results in overlapping of samples in adjacent
classes[18]. So, Sğ‘ is redefined as:
â€²

Sğ‘ =

ğ‘âˆ’1
ğ‘
1âˆ‘ âˆ‘
ğ‘ğ‘– ğ‘ğ‘— Î”2ğ‘–ğ‘— (ğœ‡ğ‘– âˆ’ ğœ‡ğ‘— )(ğœ‡ğ‘– âˆ’ ğœ‡ğ‘— )ğ‘‡
ğ‘› ğ‘–=1 ğ‘—=ğ‘–+1

(1)

When two data share the same class label, it is with high
confidence that they live on a same manifold. Therefore, the
weight value should relatively be large.
Now, the new discriminant objective of LFDA is embodied
as that it preserves the within-manifold scatter between each
sample in ğ‘ğ‘¤ to its k-nearest intra-class neighbors and maximizes the between-manifold scatter between each sample in ğ‘ğ‘
to its k-nearest interclass neighbors, which can be formulated
as
âˆ‘
â€²
max âˆ¥yğ‘– âˆ’ yğ‘— âˆ¥2 Wğ‘™ğ‘,ğ‘–ğ‘—
ğ‘–ğ‘—
âˆ‘
(6)
â€²
ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ
âˆ¥yğ‘– âˆ’ yğ‘— âˆ¥2 Wğ‘™ğ‘¤,ğ‘–ğ‘— = 1

âˆš
(ğœ‡ğ‘– âˆ’ ğœ‡ğ‘— )ğ‘‡ (ğœ‡ğ‘– âˆ’ ğœ‡ğ‘— ), and all classes are
where Î”ğ‘–ğ‘— =
treated fairly to avoid the leading role of edge classes in eigendecomposition.
Then, the improved maximum margin criterion (iMMC) can
be obtained and rewritten in the following form
â€²

arg max ğ‘¡ğ‘Ÿ(Vğ‘‡ (Sğ‘ âˆ’ Sğ‘¤ )V)
V

(2)

Based on the above discussion, we will describe our method
in more detail on next subsection.

ğ‘–ğ‘—

93

(a) synthetic data set 0

(b) synthetic data set 1

(c) synthetic data set 2

Fig. 1. Examples of dimensionality reduction by LFDA and MMC. Two-dimensional two-class samples are embedded into a one-dimensional space. The
line in the figure denotes the one-dimensional embedding space (which the data samples are projected on) obtained by each method.

between-manifold scatter between each sample in ğ‘ğ‘ to its knearest interclass neighbors simultaneously. Through a linear
weighted technique of evaluation function method, it can be
changed into the following constrained problem

With some algebraic deduction, Eq. (6) can be simplified as
âˆ‘
â€²
1
âˆ¥yğ‘– âˆ’ yğ‘— âˆ¥2 Wğ‘™ğ‘,ğ‘–ğ‘—
2
ğ‘–ğ‘—
âˆ‘
â€²
= 12 (Vğ‘‡ xğ‘– âˆ’ Vğ‘‡ xğ‘— )(Vğ‘‡ xğ‘– âˆ’ Vğ‘‡ xğ‘— )Wğ‘™ğ‘,ğ‘–ğ‘—
ğ‘–ğ‘—
(7)
âˆ‘ â€²
= ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘’(Vğ‘‡ ( Wğ‘™ğ‘,ğ‘–ğ‘— (xğ‘– âˆ’ xğ‘— )(xğ‘– âˆ’ xğ‘— )ğ‘‡ )V)

â€²

V

= ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘’(V Sğ‘™ğ‘ V)

(8)

ğ‘–ğ‘—

Similarly, the constrain in eq. (6) can be simplified as
âˆ‘
â€²
1
âˆ¥yğ‘– âˆ’ yğ‘— âˆ¥2 Wğ‘™ğ‘¤,ğ‘–ğ‘—
2
ğ‘–ğ‘—
âˆ‘
â€²
= 12 (Vğ‘‡ xğ‘– âˆ’ Vğ‘‡ xğ‘— )(Vğ‘‡ xğ‘– âˆ’ Vğ‘‡ xğ‘— )Wğ‘™ğ‘¤,ğ‘–ğ‘—
ğ‘–ğ‘—
(9)
âˆ‘ â€²
= ğ‘¡ğ‘Ÿ(Vğ‘‡ ( Wğ‘™ğ‘¤,ğ‘–ğ‘— (xğ‘– âˆ’ xğ‘— )(xğ‘– âˆ’ xğ‘— )ğ‘‡ )V)

By the Lagrange multiplier method, assuming
â€²

â€²

â€²

(10)

xğ‘– âˆ’â†’ yğ‘– = Vğ‘‡ xğ‘–
V = [v1 , v2 , . . . , vğ‘‘ ]

â€²

â€²

ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ Vğ‘‡ Sğ‘™ğ‘¤ V = 1

(11)

V

V

â€²

ğ‘‡

(16)

(17)

The LGMDA algorithm is summarized below.

Now, in order to integrate the recognition characteristics of
different classes, we combine the two solution as shown in
Eqs. (2),(11) and get a multi-optimization problem as
â§
â€²
â¨ max V ğ‘‡ Sğ‘™ğ‘ V
â© max V ğ‘‡ (Sğ‘ âˆ’ Sğ‘Š )V

â€²

Let the column vectors v1 , v2 , . . . , vğ‘‘ be the solutions of
(16), ordered according to their eigenvalues ğœ†1 â‰¥ ğœ†2 â‰¥ . . . â‰¥
ğœ†ğ‘‘ . Thus, the embedding is given as follows:

Then, the objective function of Eq. (6) can be rewritten as
the following:
V

â€²

[ğ›¼Sğ‘™ğ‘ + (1 âˆ’ ğ›¼)(Sğ‘ âˆ’ Sğ‘Š )]v = ğœ†Sğ‘™ğ‘¤ v

ğ‘–ğ‘—

max V Sğ‘™ğ‘ V

(15)

Let the derivative with respect to V to zero. The projection
vector v that maximizes (14) is given by the maximum
eigenvalue solution to the generalized eigenvalue problem:

ğ‘–ğ‘—

â€²

ğ‘‡

â€²

L(V, ğœ†) = V ğ‘‡ [ğ›¼Sğ‘™ğ‘ + (1 âˆ’ ğ›¼)(Sğ‘ âˆ’ Sğ‘Š )]Vâˆ’
â€²
ğœ†(Vğ‘‡ Sğ‘™ğ‘¤ V âˆ’ 1)

= ğ‘¡ğ‘Ÿ(Vğ‘‡ Sğ‘™ğ‘¤ V)
where Sğ‘™ğ‘¤ is the within-manifold scatter matrix as
âˆ‘ â€²
â€²
Sğ‘™ğ‘¤ =
Wğ‘™ğ‘¤,ğ‘–ğ‘— (xğ‘– âˆ’ xğ‘— )(xğ‘– âˆ’ xğ‘— )ğ‘‡

(13)

where ğ›¼ âˆˆ [0, 1] is a trade-off parameter, LGMDA is reduced
to LFDA when ğ›¼ = 1.
With some algebraic deduction, Eq. (13) can be simplified
as
â€²
â€²
max V ğ‘‡ [ğ›¼Sğ‘™ğ‘ + (1 âˆ’ ğ›¼)(Sğ‘ âˆ’ Sğ‘Š )]V
V
(14)
â€²
ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ Vğ‘‡ Sğ‘™ğ‘¤ V = 1

â€²

where Sğ‘™ğ‘ is the between-manifold scatter matrix as
âˆ‘ â€²
â€²
Sğ‘™ğ‘ =
Wğ‘™ğ‘,ğ‘–ğ‘— (xğ‘– âˆ’ xğ‘— )(xğ‘– âˆ’ xğ‘— )ğ‘‡

â€²

ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ Vğ‘‡ Sğ‘™ğ‘¤ V = 1

ğ‘–ğ‘—

â€²

ğ‘‡

â€²

max ğ›¼V ğ‘‡ Sğ‘™ğ‘ V + (1 âˆ’ ğ›¼)V ğ‘‡ (Sğ‘ âˆ’ Sğ‘Š )V

Algorithm Outline: LGMDA
Input:
1. Training samples {(x1 , â„“1 ), . . . , (xğ‘› , â„“ğ‘› )} âˆˆ â„œğ‘šÃ—ğ‘› .
2. Dimensionality of embedding space d (1 â‰¤ ğ‘‘ â‰¤ ğ‘š).
Output:
1. The optimal transformation matrix V âˆˆ â„œğ‘šÃ—ğ‘‘ .
2. The d-dimensional embedding coordinates Y for the input
points X.
Algorithm:

(12)

â€²

ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ V Sğ‘™ğ‘¤ V = 1
The constrained multi-object optimized function is intent
on maximizing the margin between different classes and the

94

Step 1: Compute the within-class scatter
matrix Sğ‘Š and the
â€²
between-class scatter matrix Sğ‘ according to Eq. (1).
Step 2: Construct neighborhood
graph G and calculate the
â€²
affinity matrix A .
Step 3: Construct within-manifold graph Gğ‘¤ and betweenthe local betweenmanifold graph Gğ‘ ; compute
â€²
manifold weight matrix Wğ‘™ğ‘ and the local withinâ€²
manifold weight matrix Wğ‘™ğ‘¤ according to Eqs. (4)
and (5).
Step 4: Construct
the local between-manifold scatter matrix
â€²
â€²
Sğ‘™ğ‘ and the local within-manifold scatter matrix Sğ‘™ğ‘¤
as given in Eqs. (8) and (10).
Step 5: Calculate the eigenvectors
corresponding
to the
â€²
â€²
largest eigenvalues of [ğ›¼Sğ‘™ğ‘ + (1 âˆ’ ğ›¼)(Sğ‘ âˆ’ Sğ‘Š )]v =
â€²
ğœ†Sğ‘™ğ‘¤ v and use them to form the optimal projection
vector V.
Step 6: The coordinates of the n points in the d-dimensional
embedding space are given by the column vectors of
Y = Vğ‘‡ X.

high-dimensional objects into two-dimensional space, and the
comparison results are illustrated in Fig. 3.
Fig. 3(a)-(f) shows that LGMDA and LFDA produce better
results than other methods, and LGMDA performs the best.
The failure of PCA may be due to its unsupervised nature,
and the poor performance of LDA and SLPP is caused by the
multiple submanifolds image data. LFDA presents the almost
comparative performance as LGMDA in that it combines the
ideas of FDA and LPP to solve the multimodal problem,
but the discriminative margin in LFDA is determined by the
limited nearby data pairs, thus produces some overlapped
point. LGMDA cannot only find the sub-manifolds of highdimensional image space, but also enforces separation of
overlapping samples from different classes, and it achieves
the best performance.
B. Image recognition
We test our method and the state-of-the-art methods on
the mentioned two databases. Some processes during the
experiment are marked as follows:
(I) For each image in COIL-20 database, we manually crop
the ear image to size of 32 Ã— 32 pixels. For each image in
YaleB database, the facial area is cropped manually from the
original image and resized to a resolution of 32 Ã— 32 pixels
with 256 gray levels. Thus, each image can be represented by
a m-dimensional vector in image space. We lastly normalize
them to be zero-mean and unit-variance vectors.
(II) We split the data into training set and test set. The
training set is used to learn a lower-dimensional embedding
space using the different methods, and all the testing samples
are projected onto the lower-dimensional embedding space.
Then, a k-nearest neighbor classifier is performed in the
optimal embedding space for classification. In the experiments,
the number of nearest neighbors (k) is taken to be 1. For
those methods need PCA to reduce dimensionality firstly, the
number of principal components is decided by remaining 98%
energy.
The experimental details are discussed as follows:
(1) Experiment on the COIL-20 database: In this experiment, we randomly select i (i= 2, 4, 6, 8) samples of each
individual for training and the others for testing. We repeat
the classification process 20 times by using different splits
and calculate the average recognition rates. Fig. 4 shows the
recognition curves of various methods.
According to Fig. 4, we can obtain the following conclusion.
For all methods, the recognition accuracy increases with the
sample size of the training set. The reason may be that a large
set of training data can sample the underlying distribution
more accurately than a smaller set. In most cases, supervised
methods perform better than unsupervised methods, and the
reason may be that supervised method can make use of
discriminant information in the image space. Our LGMDA
method is the top performer in all the experimental cases,
which indicates that LGMDA is more effective than other
methods in extracting and representing image features for
recognition over the variation of angel.

III. E XPERIMENTS AND D ISCUSSION
In this section, we evaluate the proposed algorithm on two
well-known image databases, and the typical images of two
databases are shown in Fig. 2.
1) COIL-20[19]: Columbia Object Image Library is a
database of gray scale images of 20 objects. The objects
were placed on a motorized turntable against a black
background. The turntable was rotated through 360âˆ˜
to vary object pose with a fixed camera. Images of
the objects were taken at pose intervals of 5âˆ˜ . This
corresponds to 72 images per object. Here we conduct
experiments on the second set which contains 1, 440 size
normalized images of 20 objects. Each image is resized
to 128 Ã— 128. Despite high dimensionality, the images
in this database are approximately parameterized by one
degree of freedom, e.g. the angle of rotation.
2) YaleB[20]: The YaleB face database contains 16, 128
images of 38 human subjects under 9 poses and 64
illumination conditions. In this experiment, a subset,
which contains 2, 414 near frontal gray-scale images
of 38 individuals with 68 images per individual, is
randomly selected to test the performances of LGMDA
on large database.
A. Embedding of multiple objects
In this section, we use COIL-20 dataset to illustrate the discriminant capability and dimensionality reduction capability
of LGMDA with the several representative dimensionality reduction algorithms, including PCA, FDA, MMC[5], SLPP[11],
and LFDA[15].
In this experiment, we choose five objects out of the 20
objects in COIL-20 dataset. We randomly select 10 samples
of each object for training and the others for embedding

95

(b) YaleB

(a) COIL-20
Fig. 2.
42
4 444 42242
44 2 2 224424 2
4
44444444
4
22
44
3 3 3
3
424
2 22
3 33333
24
4
5 3 5 5 5 55
3 5 35 53 3 333 3
11 441 1 1 1 1111111122
3
5
141
3
22 22 2
5
114 2
33333
555
15
114111
522 2
23
22
333
555 33
4121 44
5
1 44
2
44441
5
4 424
4 5 3
444
555 33 33
3 33
5
11 44 4444
5
3 3 555
5
3
3
5555
553 3
35 5 3 3
3
11
2
3
35
5
5
1
3
5
5
5
3 5
5
1
3 355 5 5
2
5
5
5
3
5
5
1 1
3
2
5
5
1 1
5
2
5
2
5
1 1
2
1
2
1
2
2 22 2222
1
22
1 1 12 11111
2
1
2
1
1 121
11 1 1
22 2

Sample examples of the two image databases.
1
5515 11
3
5555515 111 1
5 5
5
555
5 55 15111 1 1
555
531 511151
1
51 5
5 555
1
5 511
51 111 1511
1 15111511 111
5 5 55
353 555
11 1 11 5 5 5
111 5
3
5
333 11311 313 1
3
3
3
3
5
5
3
33333333333333 333 3
55
3333 3 33 3 35
3 3
3 33 3 3
33

2
2
2
2
44
4 44 4 4444
44444
44444
4
4
444444
4
44
4444444
44
44
44

(a)

33
333
333
33 3
33
33
333 3
333333
3 33 33333 3
3 3 333 3
3
3
33
3 33
3
3
3
2
2 2 2 2
2 2 2 2
22 2 22
22
222
222
2
2 2 22222222
2 22222
2
22
2222 2 2

2
3
3
3
4 4 222 2
42 2
4444 442
22
2 2
2
444
4 2 22 2
5
44444
55 5 3
5
42222 2 5 2
5555555
442
5
4444
55
444
44424
5
44
44
22442 22 2
5
4 2222
2
5 55 5
24
5 55 5
5
222422
44
44 2422
24
55 3 55
2222 1 1
5
5
2 442
55 5555555
222 1 111
5
3 5555
5
111 11
1 1 1
5 55 5
1
1
1
5 5 55
1111 11111
1111
111111
11
1
1
11 1 1
1
1 1
11
1111

2

(b)

222
444
4444444 444444
2
444
4444
44
4444444444444
44 44
4444
444 2222222
2 2
33333 3
2 2
3333
3 3 33
3333333333
22 2
3 3 3 33 3
2
33 33 33
2
22
3
22
3 3
22
5
5
5
555 555 25 233 323
5
5
5
5
55
2 2 333 2
22222 5 5 5 5 55 5555
5
2 5
5
5
5
252252
5
2
5
5
5
5
2
3
5
2
5
5
1
2
2 2 22 222
22
5
5 5
1111
5
533222
55
1 1
11 1
11111
111 11
1
11111 1 1 11111
1 11
1 111
111111 1
1 111111111

3
11
1
1 11
111
1
1 1
1
1 111
1111
11
11
1 1111111
11111 111
1 111111
1
1
1
111

(c)
11
1 1
11111 111
1 111111
1
111111
1
1111111
111
111 1
1
1

33
333
3333
3333
33
33333
333333
3 33 3 3 33
33
3 333 3
3
3
3
3 33 3
5 55555 33
2 2
3
3
55555555553
2
555355
55 5
55553
3
2
5 5
5555 55
55
55
5
2
5
55 5 5
5 555
222
5
2 22
22 2
2
2
22222 2
222
2 2 22
2
2
22
2 222
222
44
2 22 222222
4 444444
2
444 4
4 4
4
4
4
44
4
4
4
4
444 4
4444
444
4
44444
4
4444

(d)

4
44444
4 4
44444
4444
444
4444444 4
2
4 44
4444
2
2
4
4 44
4 4
2
2
2
222
2222222222222 2
2
2
2
22222 2 2 2
2
2
2 222 22 2
22 2
2

(e)

5

5
5 5 55
5
3
5
5 55355
5555
5
55
555 55555555
5
55
5 55553555 35
55
5
5 5
5

3

3
3
3
3 3 3 3333
3 333
33
3 3 3333 3
3 333 3
33333
3
3
3333
3

(f)

Fig. 3. Embedding results of the images of five chosen objects(red number 1, blue number 2, green number 3, yellow number4 and cyan number 5) in
COIL-20 dataset. (a) PCA; (b) LDA; (c) SLPP;(d) MMC; (e) LFDA; (f) LGMDA.

(a)

(b)

(c)

(d)

Fig. 4. Average recognition accuracy vs. dimension of different methods on the COIL-20 database. (a) Two images of each individual for training; (b) Four
images of each individual for training; (c) Six images of each individual for training; (d) Eight images of each individual for training.

(a)

(b)

(c)

(d)

Fig. 5. Average recognition accuracy vs. dimension of different methods on the YaleB database. (a) Two images of each individual for training; (b) Four
images of each individual for training; (c) Six images of each individual for training; (c) Eight images of each individual for training.

Fig. 5.

(2) Experiment on the YaleB database: In this experiment, a
subset, which contains 760 gray-scale images of handwritten
digits with 20 images per individual, is randomly selected to
test the performances of LGMDA on large database. In the
YaleB subset, i (i= 2, 4, 6, 8) images of each individual are
randomly selected for training, while the remaining images are
used for testing. All the results are 20 times averaged by using
different splits. The average recognition results are shown in

Based on the experimental results in Fig. 5, in most cases,
LGMDA outperforms other methods, and SLPP, LSDA, LFDA
are better than PCA+FDA and MMC. FDA and MMC discover
only the Euclidean structure, and cannot reveal the underlying
nonlinear manifolds that facial images lie on. Therefore, their
discriminating power is limited. SLPP, LSDA and LFDA are
supervised manifold learning methods and try to discover

96

the intrinsic manifold structure of the facial image space.
Their discriminating powers are better than FDA and MMC.
LGMDA provides the best projection since it tries to separate
labeled samples in different classes from each other, while it
can preserve the manifold structure of data.

proposed algorithm is verified by experiments on two wellknown image databases.
ACKNOWLEDGMENT
This work is partially supported by the Natural Science
Foundation Project (CSTC2009BB2195), the Key Science
Projects (CSTC2009AB2231) of Chongqing Science, China.
The authors would like to thank the anonymous reviewers for
their constructive advice.

C. Discussion
The experiments on the two image databases have revealed
some interesting points.
âˆ™ For all methods, the recognition accuracy increases with
the sample size of the training set. The reason may be
that a large set of training data can sample the underlying
distribution more accurately than a smaller set.
âˆ™ Our LGMDA method is the top performer in all the
experimental cases, which demonstrates that LGMDA
is more effective than other methods in extracting and
representing image features for recognition in two image databases. The good performance of the proposed
method indicates that LGMDA combines global discriminant measure and local clustering information together
to achieve good discrimination for better classification.
The global discriminant measure enforces separation of
overlapping samples from different classes and local
discriminant efforts compact data in the same class.
âˆ™ An effective learning method can achieve good performance when the size of training sample is small. As
expected, the recognition rate of LGMDA is significantly
better than other methods when the size of training
images is small in Figs. 4-5. As shown in Fig. 5, it is
clear that the recognition rates of PCA+LDA and MMC
on YaleB database are significantly degraded than those
on COIL-20 database, which indicates that PCA+FDA
and MMC cannot extract the intrinsic discriminant features for face recognition when the facial images vary
in lighting and pose conditions as well as the image
database becomes large. FDA and MMC discover only
the Euclidean structure, and cannot reveal the underlying
nonlinear manifolds that facial images lie on. Therefore
their discriminating power is limited. However, LGMDA
can extract discriminant features for both discriminating between-class manifolds and preserving within-class
manifold structure, so it achieves more promising results
on COIL-20 and YaleB image databases.

R EFERENCES
[1] Huang H, Li J W, Feng H L. Subspaces versus submanifolds: a comparative study in small sample size problem. Int. J. Pattern Recognition and
Artificial Intelligence, 2009, 23(3): 463-490.
[2] Lu G F, Lin Z, Jin Z. Face recognition using discriminant locality preserving projections based on maximum margin criterion. Pattern Recognition,
2010, 43(10): 3572-3579.
[3] Cheng M, Fang B, Wen J, Tang Y Y. Marginal discriminant projections:
An adaptable margin discriminant approach to feature reduction and
extraction. Pattern Recognition Letters, 2010, 31(13): 1965-1974.
[4] Yang J, Zhang D, Yang J Y, Niu B. Globally maximizing, locally
minimizing: Unsupervised discriminant projection with applications to
face and palm biometrics. IEEE Trans. Pattern Anal Mach Intell, 2007,
29(4): 650-664.
[5] Li H, Jiang T, and Zhang K. Efficient and robust feature extraction by
maximum margin criterion. IEEE Trans. Neural Networks, 2006, 17 (1):
157-165.
[6] Tenenbaum J B, Silva V de, Langford J C. A global geometric framework
for nonlinear dimensionality reduction. Science, 2000, 290 (5500): 23192323.
[7] Roweis S T and Saul L K. Nonlinear dimensionality reduction by locally
linear embedding. Science, 2000, 290 (5500):2323-2326.
[8] Belkin M and Niyogi P. Laplacian eigenmaps for dimensionality reduction
and data representation. Neural Computation, 2003, 15 (6): 1373-1396.
[9] Zhang Zh Y and Zha H Y. Principal manifolds and nonlinear dimension reduction via Local Tangent Space Alignment. SIAM J. Scientific
Computing, 2005, 26 (1): 313-338.
[10] He XF, Yan SC, Hu Y, et al. Face recognition using Laplacianfaces.
IEEE Trans. Pattern Anal Mach Intell, 2005, 27 (3): 328-340.
[11] He X F and Niyogi P. Locality preserving projections. in: Proceedings
of Conf. Advances in Neural Information Processing System, Vancouver,
Canada, 2003.
[12] He X F, Cai D, and Yan S C, et al. Neighborhood preserving embedding.
in: Proceedings of 10th Int. Conf. Computer Vision,2005, pp. 1208-1213.
[13] Yan S C, Xu D, Zhang B, et al. Graph embedding and extensions: a
general framework for dimensionality reduction. IEEE Trans Pattern Anal
Mach Intell, 2007, 29 (1): 40-51.
[14] Cai D, He X F, and Zhou K. Locality sensitive discriminant analysis. in:
Proceedings of Int. Conf. Artificial Intelligence, Hyderabad, India, 2007,
pp. 708-713.
[15] Sugiyama M. Dimensionality reduction of multimodal labeled data by
local Fisher discriminant analysis. J. Machine Learning Research, 2007,
8: 1027-1061.
[16] Han P Y, Jin A T, Abas F S. Neighbourhood preserving discriminant
embedding in face recognition. J. Vis. Commun. Image R. 2009, 20 (8):
532-542
[17] Fang B, Cheng M, Tang Y Y, He G H. Improving the discriminant
ability of local margin based learning method by incorporating the
global between-class separability criterion. Neurocomputing, 2009, 73(1):
536C541
[18] Ma X H, Tan Y Q, Zhao Y Y, Tian H B. Face Recognition Based
on Maximizing Margin and Discriminant Locality Preserving Projection.
Journal of Information & Computational Science,2010, 7(7): 1551-1559.
[19] Nene SA, Nayar SK, Murase H. Columbia Object Image Library (COIL20). Technical Report, Department of Computer Science, Carnegie Mellon
University, 1996.
[20] Georghiades A S, Belhumeur P N, and Kriegman D J. From Few to
Many: Illumination Cone Models for Face Recognition under Variable
Lighting and Pose. IEEE Trans. Pattern Anal Mach Intell, 2001, 23(6):
643-660.

IV. C ONCLUSIONS
As introduced in this paper, either MMC or LFDA has its
distinctive advantage for discriminative subspace learning. To
overcome the intrinsic limitation of LFDA, a new method,
called local and global marginal discriminant analysis method
(LGMDA), is proposed for image recognition. LGMDA combines the maximum margin criterion into the local margin
criterion utilized in LFDA to achieve good discrimination for
better classification. The global discriminant measure in the
maximum margin criterion enforces separation of overlapping
samples from different classes and local discriminant efforts
compact data in the same class. The effectiveness of the

97

