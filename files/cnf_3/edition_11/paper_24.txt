A Multimodal Image Fusion Framework Applied in Radiotherapy

N. Riefenstahl", G. &ell*, R. Calow*, B. Michaelis*, M. Walke**
*Institutefor Electronics, Signal Processing and Communications, Otto-von-Guericke University
Magdeburg, Germany
**Clinicfor Ratdiotherapy,Otto- von- Guericke- University Magdeburg, Germany
i.iefetistuhl@iesk.et.uiii-mcigdebirr~.tie

the requirement to immediately correct the patient set-up if
he is displaced.

Abstract
The monitoring of patients position during treatment
in radiotherapy requires suitable real-time visualization
tools which provide a maximum of information. Often, only
Electronic Portal Images (EPls), which are of poor
quality, exist for the dynamical verification of position
deviations, whereas static image material of higher quality
is available from diagnostics (and treatment planning. A
framework for the fusion of static and dynamic multimodal
image data is proposed, which is based on the several
image sources existing in radior'herapy.

2. Background
In radiotherapy, 2-D and 3-D images exist from
several sources, acquired at different times. The available
images show different aspects of the same anatomical
region.
The CT device is one of the most important tools in
diagnosis and treatment planning, providing volumetric 3D data of the patients body with high quality. Digitally
Reconstructed Radiographs (DRR) can be calculated by
projection from CT volume data tc get a 2-D beams eye
view. In addition to this, so called Topograms are available
from CT device, which represent high resolution views
from lateral directions and normally used as scout views
for the definition of regions of interest.
One step of the treatment planning phase is a tumor
localization in a simulator room at diagnostic energy.
Results of this treatment simulation are usually film (Xray) images. With additional equipment for digital image
acquisition, simulator images (SI) can also be acquired
directly from an intensifier.
With the integration of modern electronic portal imaging
devices (EPIDs) in radiotherapy, digital on-line images are
available in real time or near real time during irradiation
[I]. The obtained digital images, produced as projection of
high energy X-rays, are usually the only available on-line
information of the body interior during treatment. A
registration of electronic portal images (EPIs) with static
image material from diagnostics and treatment planning
can support the position verification and is a common
practice. This can be done manually by the clinician, but
also several methods for the automatic or semi-automatic
registration of electronic portal images with simulator
images or 3-D data were introduced [ 2 ] .
A major problem of using EPIs for position verification is
their poor quality. Because of the degraded contrast at
megavoltage energies, the visibility of different tissue is

1. Introduction
The set-up and verification of the correct patient
position plays a vital role during the treatment in
radiotherapy. Powerful image based tools are therefore
necessary to provide the visual information needed for
motion monitoring during treatment and to recognize
deviations between desired and actual treatment volume.
Besides static misalignment, motion of the patient caused
for example by respiration must be considered.
Electronic Portal Images (EPI) or Megavoltage Images
(MVI) are obtained during irradiation as an X-ray
projection with treatment energy. These images are usually
the only existing image source during irradiation.
Unfortunately, the unprocessed EPIs are of poor quality
and represent only a small field of view. On the other hand,
static 2-D and 3-D images like CT, film and simulator
images exist from diagnostics and simulation that are of
good quality.
The proposed fusion framework for dynamic in-treatment
data and static pre-treatment diagnostic and simulation data
provides improved real-time visualization tools and tools
for a quantitative assessment of patient positioning.
One aim is for example the dynamic enhancement of
Electronic Portal Images by using offline image data as apriori knowledge. Real-time capability is desired to meet

0-7695-1195-3/01
$10.00 0 2001 IEEE

173

Associator Image
Optical 3-D
Surface Sensor

Common
Representation

in-treatment
data

.

- gray level image
- contours
- PTV

Electronic Portal
Image (EPI)

pre-treatment data
Figure 1: Image Fusion Framework
limited and therefore the usability of EPIs is restricted [3].
Because of the limited effects of known conventional EPI
enhancement methods, a fusion of EPIs with static pretreatment images is meaningful to obtain improved intreatment visualization tools.
EPIs provide only a 2D analysis, which is a disadvantage
in the presence of deviations in parallel to the imaging
device. To support the set-up and in-treatment monitoring
of the patient, an additional 3-D optical surface sensor
system is integrated in the framework.

important organ structures for example. This serves to
transform the images into a common presentation that is
required for the following steps.
In a second stage of the framework, a so called Associator
realizes the fusion of images based on the known
transformation parameters. With it, a combination of
several techniques is defined, providing possibilities to
associate the several images, representing different aspects.
One approach is the storage of images of good quality in
an associative memory [6]. Another approach is directly to
shift or even to warp this images according to the detected
position or shape by the optical sensor.

3. Image Fusion Framework

We use an artificial neural network called Modified
Associative Memory (MAM) for image fusion, which is
introduced in [4].It uses properties of special structured
linear neural networks, who’s parameters are determined
by Principal Component Analysis (PCA) or Hebbian
Learning [5]. Figure 2 shows the input- output structure of
the network. This MAM can be used for fusion of two data
sets. The inputs and outputs are provided by the pixel gray
values. The storage of the static image prototypes in a
training phase is performed by applying a training data set
representing all expected variations during the treatment.
The prototype images are then presented by the weights of
the hidden neurons.
In a recall phase, dynamically acquired image data is used
to recall adapted images with a higher information content.
The associative memory is able to interpolate between
similar image patterns in the training data set.
The degree of association, visible in the output
representation, can be determined by the ratio between
actual input and the stored information part. This is
adjustable for example by the chosen number of hidden
neurons of an Modified Associative Memory [4]. This can

3.1. Overview
The available pre-treatment and in-treatment image
data are fused in a two stage process. Figure 1 shows an
overview of the proposed framework.
The first stage realizes a transformation of the different
image modalities in a common representation space. For a
first approximation, these are the gray values of the images
that are produced as projections of corresponding anatomic
regions. In detail, transformation parameters of the sensor
coordinate systems of the simulation, diagnostic and
treatment devices are determined based on the well known
geometrical relations available from the treatment planning
system. This yields matching parameters between the
several image sources and therefore a coarse alignment of
visible features. Additional devices, like the optical 3-D
surface sensor, integrated in the proposed framework, must
be calibrated, e.g. based on markers visible in several
image modalities.
T o consider the different energy levels at acquisition time a
pre-processing is required. This can be a segmentation of

174

The generated surface data gives only extrinsic information
of the patients motion, which cannot be used to directly
derive motion inside the patients body. By registration of
the measured surface data and the body surface generated
from the existing CT dataset, deviations of the patients
body surface can be determined. Based on this motion
parameters, adapted DRRs can be calculated as projection
from the CT dataset for the treatment directions
determined in treatment planning. Figure 4 shows a
measured body surface mesh and two corresponding DRRs
calculated from two projection directions and under the
assumption the body is rigid. Another possibility for a
feasible output representation is a colour coded
visualization of the differences of the measured surface
data and surface generated from CT.

Figure 2: 2D input-output structure of the Modified
Associative Memory (MAM) [4]
be done interactively, e.g. by the clinician, to adapt the
visual representation.
To improve the adaptation of the recall images, a warping
algorithm is used to consider gross displacements of
structures. The warping parameters can be determined by
tracking segmented image structures.
Based on calculated tracking p,arameters, it is also possible
to influence the associative memory directly. This can be
done by changing the weights of the neural network
depending upon the detected motion parameters.

Figure 3: The optical 3-D surface sensor

3.2.3-D surface tracking
The usage of 3-D surface sensor systems i n
radiotherapy is a relatively new approach. One task of a 3D sensor is to improve the set-up and position verification
during treatment. Several techniques based on optical
sensor systems were introduced for this purpose [7] [8].
To improve the coarse alignment of'the static and dynamic
image data, we use an optical 3D- surface sensor,
consisting of a stereo camera pair and an LCD projector
[9]. Figure 3 shows a view of the sensor system installed at
the clinic. The projector is usecl to project light patterns on
the uncovered skin of the ptient's body allowing to
determine corresponding points in the two camera views.
The sensor system dynamically generates 3-D point clouds
with an appropriate calibration xcuracy of about 1 mm.

Figure 4: Measured surface mesh fused with DRRs
calculated from two projection directions

175

view and are of very low quality. As an example of a
feasible visualization, the original unprocessed EPI is
visualized in a topogram image, that is tracked with the
position detected by the optical surface sensor (see Figure
7). The dashed line marks the irradiation field.
An MAM network was trained with a dataset representing
the expected variations in shape and position. The
corresponding region of interest in the topogram was
divided into patches of 20 by 20 pixels. This decreases the
number of neurons of the network and leads to a faster
computation during the training phase. The recall was done
under different conditions concerning the dominance of the
gray values from the static pre- treatment data and the
dynamical in-treatment image data. This was realized
using MAMs with different numbers of hidden neurons.
Figure 8 shows the result of the fusion using 23 hidden
neurons. In this case, the EPI for the recall has a high
influence to the generated output representation. In the
resulting image generated with 3 hidden neurons (see
Figure 9), only small differences concerning to image
structures are visible between the original topogram and
the associative recall.

Figure 5: Phantom on a vibration table

3.3. EPI enhancement
The usability of the described image fusion framework
can be demonstrated with the real-time enhancement of
Electronic Portal Images by using static pre-treatment
image data from diagnostics and treatment planning as apriori knowledge.
To generate a training data set for the associative memory,
several suitable image sources, representing static image
data and variations, exist. Simulator images directly
represent a view of the same body region as visible in the
EPIs during treatment. Another possibility is to use DRRs
or topograms, available from the CT device or the

5. Conclusions
In the proposed framework, off-line and on-line image
data of different kind is fused in an Associator to enhance
on-line visualization during irradiation. As one example,
an tracked simulator image together with an associated EPI
can be visualized.

6. Acknowledgements

treatment planning system.

After a coarse alignment of the interesting images based on
known parameters from treatment planning, segmented
bony structures can be tracked in the dynamically acquired
EPIs. The calculated tracking parameters are used for a
warping algorithm, which yields adapted images for the
recall of the associative memory.

This work was supported by an EU grant for the
ARROW project (a BIOMED 2 project, BMH4983660)
and by the federal state Sachsen- Anhalt (FKZ
0002 KE0099).

4. Results
For experiments in dynamic fusion of image data, a
test object (phantom) was used. Together with a specially
developed motor driven vibration table it was possible to
simulate patient movements under irradiation (see Figure
5) and stable repeatable conditions. Therefore, real patients
are not required for basic investigations. The vibration
table swings with adjustable frequency and an amplitude of
max. 2 cm.
For the enhancement of EPIs, a topogram image of the
phantom was used as static a-priori image data of higher
quality (see Figure 6). The phantom is completely built of
slices, which produces vertical lines in the image. The
dynamically acquired EPIs represent only a small field of

176

Original unprocessed EPI projected into
topogram image

Figure 6: Original topogram of the phantom used
for the training of the associative memory

Figure 7:

Figure 8: Recall image projected into topogram image
(MAM with 23 hidden neurons)

Figure 9: Recall image projected into topogram image
(MAM with 3 hidden neurons)

177

7.References
[ I ] Boyer, A.L., Antonuk, L., Fenster, A., van Herk, M.,
Meertens, H. Munro, P. Reinstein, L. E. and Won, J. 1992; A
review of electronic portal imaging devices (EPIDs), Med. Phys.
19 1-16

[6] G. Krell, B. Michaelis, G. Gademann: Image Fusion by an
Associative Memory Applied in Radiotherapy, Proc. of the
Intemational Congress on Intelligent Systems & Applications
ISA’2000, Dec. 12-15 Wollongong, pp. 1544-1 549

[2] Gilhuijs, K.G.A., van den Ven, P.J.H., van Herk,
M.:Automatic three dimensional inspection of patient setup in
radiation therapy using digital portal images and computed
tomography data., Med. Phys., Vol. 23, No. 3, pp 389-399, 1996

[7] Walke, M., Albrecht, P., Carlo, R., Krell, G., Gademann, G.,
Michaelis, B: Patient Position And Motion Investigations With A
New Optical Three Dimensional Sensor. Proceedings of the
European Medical & Biological Engineering Conference
EMBEC’99, Vienna, 1999

[3] Smith, A. R. et al: Radiation Therapy Physics (Medical
Radiology), Springer Verlag, 1996

[8] Baroni, G., Giancarlo, F. Orecchia, R., Pedotti, A., Real-time
three dimensional motion analysis for patient positioning
verification, Radiotherapy and Oncology, Vol. 54 (1) (2000)
pp. 2 1-27, 2000

[4] Krell, G.; Tizhoosh, H. R.; Lilienblum, T.; Moore, C.J.,
Michaelis, B.: Fuzzy Image Enhancement and Associative
Feature Matching in Radiotherapy. Proceedings of the
lntemational Conference on Neural Networks (ICNN ‘97),
Houston, Texas, 09.06,-12.06.1997, pp. 1490-1495

[9] Albrecht P, Michaelis B: Improvement of the Spatial
Resolution of an Optical 3-D Measurement Procedure. IEEE
Transactions on Instrumentation and Measurement, Vol47, No 1,
158-162, Februar 1998.

[ 5 ] Oja, E, Ogada, H, Wangviwattana, J: Learning in nonlinear,
constained Hebbian networks. In: Kohonen et al. (eds), Artificial
Neural Networks, Proc ICANNP 1, North-Holland, Amsterdam,
385-375, 1991

178

